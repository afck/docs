<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <meta name="description" content="Source to the Rust file `src/lib.rs`.">
    <meta name="keywords" content="rust, rustlang, rust-lang">

    <title>lib.rs.html -- source</title>

    <link rel="stylesheet" type="text/css" href="../../main.css">

    
    
</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    

    <section class="sidebar">
        
        
    </section>

    <nav class="sub">
        <form class="search-form js-only">
            <div class="search-container">
                <input class="search-input" name="search"
                       autocomplete="off"
                       placeholder="Click or press 'S' to search, '?' for more options..."
                       type="search">
            </div>
        </form>
    </nav>

    <section id='main' class="content source"><pre class="line-numbers"><span id="1">   1</span>
<span id="2">   2</span>
<span id="3">   3</span>
<span id="4">   4</span>
<span id="5">   5</span>
<span id="6">   6</span>
<span id="7">   7</span>
<span id="8">   8</span>
<span id="9">   9</span>
<span id="10">  10</span>
<span id="11">  11</span>
<span id="12">  12</span>
<span id="13">  13</span>
<span id="14">  14</span>
<span id="15">  15</span>
<span id="16">  16</span>
<span id="17">  17</span>
<span id="18">  18</span>
<span id="19">  19</span>
<span id="20">  20</span>
<span id="21">  21</span>
<span id="22">  22</span>
<span id="23">  23</span>
<span id="24">  24</span>
<span id="25">  25</span>
<span id="26">  26</span>
<span id="27">  27</span>
<span id="28">  28</span>
<span id="29">  29</span>
<span id="30">  30</span>
<span id="31">  31</span>
<span id="32">  32</span>
<span id="33">  33</span>
<span id="34">  34</span>
<span id="35">  35</span>
<span id="36">  36</span>
<span id="37">  37</span>
<span id="38">  38</span>
<span id="39">  39</span>
<span id="40">  40</span>
<span id="41">  41</span>
<span id="42">  42</span>
<span id="43">  43</span>
<span id="44">  44</span>
<span id="45">  45</span>
<span id="46">  46</span>
<span id="47">  47</span>
<span id="48">  48</span>
<span id="49">  49</span>
<span id="50">  50</span>
<span id="51">  51</span>
<span id="52">  52</span>
<span id="53">  53</span>
<span id="54">  54</span>
<span id="55">  55</span>
<span id="56">  56</span>
<span id="57">  57</span>
<span id="58">  58</span>
<span id="59">  59</span>
<span id="60">  60</span>
<span id="61">  61</span>
<span id="62">  62</span>
<span id="63">  63</span>
<span id="64">  64</span>
<span id="65">  65</span>
<span id="66">  66</span>
<span id="67">  67</span>
<span id="68">  68</span>
<span id="69">  69</span>
<span id="70">  70</span>
<span id="71">  71</span>
<span id="72">  72</span>
<span id="73">  73</span>
<span id="74">  74</span>
<span id="75">  75</span>
<span id="76">  76</span>
<span id="77">  77</span>
<span id="78">  78</span>
<span id="79">  79</span>
<span id="80">  80</span>
<span id="81">  81</span>
<span id="82">  82</span>
<span id="83">  83</span>
<span id="84">  84</span>
<span id="85">  85</span>
<span id="86">  86</span>
<span id="87">  87</span>
<span id="88">  88</span>
<span id="89">  89</span>
<span id="90">  90</span>
<span id="91">  91</span>
<span id="92">  92</span>
<span id="93">  93</span>
<span id="94">  94</span>
<span id="95">  95</span>
<span id="96">  96</span>
<span id="97">  97</span>
<span id="98">  98</span>
<span id="99">  99</span>
<span id="100"> 100</span>
<span id="101"> 101</span>
<span id="102"> 102</span>
<span id="103"> 103</span>
<span id="104"> 104</span>
<span id="105"> 105</span>
<span id="106"> 106</span>
<span id="107"> 107</span>
<span id="108"> 108</span>
<span id="109"> 109</span>
<span id="110"> 110</span>
<span id="111"> 111</span>
<span id="112"> 112</span>
<span id="113"> 113</span>
<span id="114"> 114</span>
<span id="115"> 115</span>
<span id="116"> 116</span>
<span id="117"> 117</span>
<span id="118"> 118</span>
<span id="119"> 119</span>
<span id="120"> 120</span>
<span id="121"> 121</span>
<span id="122"> 122</span>
<span id="123"> 123</span>
<span id="124"> 124</span>
<span id="125"> 125</span>
<span id="126"> 126</span>
<span id="127"> 127</span>
<span id="128"> 128</span>
<span id="129"> 129</span>
<span id="130"> 130</span>
<span id="131"> 131</span>
<span id="132"> 132</span>
<span id="133"> 133</span>
<span id="134"> 134</span>
<span id="135"> 135</span>
<span id="136"> 136</span>
<span id="137"> 137</span>
<span id="138"> 138</span>
<span id="139"> 139</span>
<span id="140"> 140</span>
<span id="141"> 141</span>
<span id="142"> 142</span>
<span id="143"> 143</span>
<span id="144"> 144</span>
<span id="145"> 145</span>
<span id="146"> 146</span>
<span id="147"> 147</span>
<span id="148"> 148</span>
<span id="149"> 149</span>
<span id="150"> 150</span>
<span id="151"> 151</span>
<span id="152"> 152</span>
<span id="153"> 153</span>
<span id="154"> 154</span>
<span id="155"> 155</span>
<span id="156"> 156</span>
<span id="157"> 157</span>
<span id="158"> 158</span>
<span id="159"> 159</span>
<span id="160"> 160</span>
<span id="161"> 161</span>
<span id="162"> 162</span>
<span id="163"> 163</span>
<span id="164"> 164</span>
<span id="165"> 165</span>
<span id="166"> 166</span>
<span id="167"> 167</span>
<span id="168"> 168</span>
<span id="169"> 169</span>
<span id="170"> 170</span>
<span id="171"> 171</span>
<span id="172"> 172</span>
<span id="173"> 173</span>
<span id="174"> 174</span>
<span id="175"> 175</span>
<span id="176"> 176</span>
<span id="177"> 177</span>
<span id="178"> 178</span>
<span id="179"> 179</span>
<span id="180"> 180</span>
<span id="181"> 181</span>
<span id="182"> 182</span>
<span id="183"> 183</span>
<span id="184"> 184</span>
<span id="185"> 185</span>
<span id="186"> 186</span>
<span id="187"> 187</span>
<span id="188"> 188</span>
<span id="189"> 189</span>
<span id="190"> 190</span>
<span id="191"> 191</span>
<span id="192"> 192</span>
<span id="193"> 193</span>
<span id="194"> 194</span>
<span id="195"> 195</span>
<span id="196"> 196</span>
<span id="197"> 197</span>
<span id="198"> 198</span>
<span id="199"> 199</span>
<span id="200"> 200</span>
<span id="201"> 201</span>
<span id="202"> 202</span>
<span id="203"> 203</span>
<span id="204"> 204</span>
<span id="205"> 205</span>
<span id="206"> 206</span>
<span id="207"> 207</span>
<span id="208"> 208</span>
<span id="209"> 209</span>
<span id="210"> 210</span>
<span id="211"> 211</span>
<span id="212"> 212</span>
<span id="213"> 213</span>
<span id="214"> 214</span>
<span id="215"> 215</span>
<span id="216"> 216</span>
<span id="217"> 217</span>
<span id="218"> 218</span>
<span id="219"> 219</span>
<span id="220"> 220</span>
<span id="221"> 221</span>
<span id="222"> 222</span>
<span id="223"> 223</span>
<span id="224"> 224</span>
<span id="225"> 225</span>
<span id="226"> 226</span>
<span id="227"> 227</span>
<span id="228"> 228</span>
<span id="229"> 229</span>
<span id="230"> 230</span>
<span id="231"> 231</span>
<span id="232"> 232</span>
<span id="233"> 233</span>
<span id="234"> 234</span>
<span id="235"> 235</span>
<span id="236"> 236</span>
<span id="237"> 237</span>
<span id="238"> 238</span>
<span id="239"> 239</span>
<span id="240"> 240</span>
<span id="241"> 241</span>
<span id="242"> 242</span>
<span id="243"> 243</span>
<span id="244"> 244</span>
<span id="245"> 245</span>
<span id="246"> 246</span>
<span id="247"> 247</span>
<span id="248"> 248</span>
<span id="249"> 249</span>
<span id="250"> 250</span>
<span id="251"> 251</span>
<span id="252"> 252</span>
<span id="253"> 253</span>
<span id="254"> 254</span>
<span id="255"> 255</span>
<span id="256"> 256</span>
<span id="257"> 257</span>
<span id="258"> 258</span>
<span id="259"> 259</span>
<span id="260"> 260</span>
<span id="261"> 261</span>
<span id="262"> 262</span>
<span id="263"> 263</span>
<span id="264"> 264</span>
<span id="265"> 265</span>
<span id="266"> 266</span>
<span id="267"> 267</span>
<span id="268"> 268</span>
<span id="269"> 269</span>
<span id="270"> 270</span>
<span id="271"> 271</span>
<span id="272"> 272</span>
<span id="273"> 273</span>
<span id="274"> 274</span>
<span id="275"> 275</span>
<span id="276"> 276</span>
<span id="277"> 277</span>
<span id="278"> 278</span>
<span id="279"> 279</span>
<span id="280"> 280</span>
<span id="281"> 281</span>
<span id="282"> 282</span>
<span id="283"> 283</span>
<span id="284"> 284</span>
<span id="285"> 285</span>
<span id="286"> 286</span>
<span id="287"> 287</span>
<span id="288"> 288</span>
<span id="289"> 289</span>
<span id="290"> 290</span>
<span id="291"> 291</span>
<span id="292"> 292</span>
<span id="293"> 293</span>
<span id="294"> 294</span>
<span id="295"> 295</span>
<span id="296"> 296</span>
<span id="297"> 297</span>
<span id="298"> 298</span>
<span id="299"> 299</span>
<span id="300"> 300</span>
<span id="301"> 301</span>
<span id="302"> 302</span>
<span id="303"> 303</span>
<span id="304"> 304</span>
<span id="305"> 305</span>
<span id="306"> 306</span>
<span id="307"> 307</span>
<span id="308"> 308</span>
<span id="309"> 309</span>
<span id="310"> 310</span>
<span id="311"> 311</span>
<span id="312"> 312</span>
<span id="313"> 313</span>
<span id="314"> 314</span>
<span id="315"> 315</span>
<span id="316"> 316</span>
<span id="317"> 317</span>
<span id="318"> 318</span>
<span id="319"> 319</span>
<span id="320"> 320</span>
<span id="321"> 321</span>
<span id="322"> 322</span>
<span id="323"> 323</span>
<span id="324"> 324</span>
<span id="325"> 325</span>
<span id="326"> 326</span>
<span id="327"> 327</span>
<span id="328"> 328</span>
<span id="329"> 329</span>
<span id="330"> 330</span>
<span id="331"> 331</span>
<span id="332"> 332</span>
<span id="333"> 333</span>
<span id="334"> 334</span>
<span id="335"> 335</span>
<span id="336"> 336</span>
<span id="337"> 337</span>
<span id="338"> 338</span>
<span id="339"> 339</span>
<span id="340"> 340</span>
<span id="341"> 341</span>
<span id="342"> 342</span>
<span id="343"> 343</span>
<span id="344"> 344</span>
<span id="345"> 345</span>
<span id="346"> 346</span>
<span id="347"> 347</span>
<span id="348"> 348</span>
<span id="349"> 349</span>
<span id="350"> 350</span>
<span id="351"> 351</span>
<span id="352"> 352</span>
<span id="353"> 353</span>
<span id="354"> 354</span>
<span id="355"> 355</span>
<span id="356"> 356</span>
<span id="357"> 357</span>
<span id="358"> 358</span>
<span id="359"> 359</span>
<span id="360"> 360</span>
<span id="361"> 361</span>
<span id="362"> 362</span>
<span id="363"> 363</span>
<span id="364"> 364</span>
<span id="365"> 365</span>
<span id="366"> 366</span>
<span id="367"> 367</span>
<span id="368"> 368</span>
<span id="369"> 369</span>
<span id="370"> 370</span>
<span id="371"> 371</span>
<span id="372"> 372</span>
<span id="373"> 373</span>
<span id="374"> 374</span>
<span id="375"> 375</span>
<span id="376"> 376</span>
<span id="377"> 377</span>
<span id="378"> 378</span>
<span id="379"> 379</span>
<span id="380"> 380</span>
<span id="381"> 381</span>
<span id="382"> 382</span>
<span id="383"> 383</span>
<span id="384"> 384</span>
<span id="385"> 385</span>
<span id="386"> 386</span>
<span id="387"> 387</span>
<span id="388"> 388</span>
<span id="389"> 389</span>
<span id="390"> 390</span>
<span id="391"> 391</span>
<span id="392"> 392</span>
<span id="393"> 393</span>
<span id="394"> 394</span>
<span id="395"> 395</span>
<span id="396"> 396</span>
<span id="397"> 397</span>
<span id="398"> 398</span>
<span id="399"> 399</span>
<span id="400"> 400</span>
<span id="401"> 401</span>
<span id="402"> 402</span>
<span id="403"> 403</span>
<span id="404"> 404</span>
<span id="405"> 405</span>
<span id="406"> 406</span>
<span id="407"> 407</span>
<span id="408"> 408</span>
<span id="409"> 409</span>
<span id="410"> 410</span>
<span id="411"> 411</span>
<span id="412"> 412</span>
<span id="413"> 413</span>
<span id="414"> 414</span>
<span id="415"> 415</span>
<span id="416"> 416</span>
<span id="417"> 417</span>
<span id="418"> 418</span>
<span id="419"> 419</span>
<span id="420"> 420</span>
<span id="421"> 421</span>
<span id="422"> 422</span>
<span id="423"> 423</span>
<span id="424"> 424</span>
<span id="425"> 425</span>
<span id="426"> 426</span>
<span id="427"> 427</span>
<span id="428"> 428</span>
<span id="429"> 429</span>
<span id="430"> 430</span>
<span id="431"> 431</span>
<span id="432"> 432</span>
<span id="433"> 433</span>
<span id="434"> 434</span>
<span id="435"> 435</span>
<span id="436"> 436</span>
<span id="437"> 437</span>
<span id="438"> 438</span>
<span id="439"> 439</span>
<span id="440"> 440</span>
<span id="441"> 441</span>
<span id="442"> 442</span>
<span id="443"> 443</span>
<span id="444"> 444</span>
<span id="445"> 445</span>
<span id="446"> 446</span>
<span id="447"> 447</span>
<span id="448"> 448</span>
<span id="449"> 449</span>
<span id="450"> 450</span>
<span id="451"> 451</span>
<span id="452"> 452</span>
<span id="453"> 453</span>
<span id="454"> 454</span>
<span id="455"> 455</span>
<span id="456"> 456</span>
<span id="457"> 457</span>
<span id="458"> 458</span>
<span id="459"> 459</span>
<span id="460"> 460</span>
<span id="461"> 461</span>
<span id="462"> 462</span>
<span id="463"> 463</span>
<span id="464"> 464</span>
<span id="465"> 465</span>
<span id="466"> 466</span>
<span id="467"> 467</span>
<span id="468"> 468</span>
<span id="469"> 469</span>
<span id="470"> 470</span>
<span id="471"> 471</span>
<span id="472"> 472</span>
<span id="473"> 473</span>
<span id="474"> 474</span>
<span id="475"> 475</span>
<span id="476"> 476</span>
<span id="477"> 477</span>
<span id="478"> 478</span>
<span id="479"> 479</span>
<span id="480"> 480</span>
<span id="481"> 481</span>
<span id="482"> 482</span>
<span id="483"> 483</span>
<span id="484"> 484</span>
<span id="485"> 485</span>
<span id="486"> 486</span>
<span id="487"> 487</span>
<span id="488"> 488</span>
<span id="489"> 489</span>
<span id="490"> 490</span>
<span id="491"> 491</span>
<span id="492"> 492</span>
<span id="493"> 493</span>
<span id="494"> 494</span>
<span id="495"> 495</span>
<span id="496"> 496</span>
<span id="497"> 497</span>
<span id="498"> 498</span>
<span id="499"> 499</span>
<span id="500"> 500</span>
<span id="501"> 501</span>
<span id="502"> 502</span>
<span id="503"> 503</span>
<span id="504"> 504</span>
<span id="505"> 505</span>
<span id="506"> 506</span>
<span id="507"> 507</span>
<span id="508"> 508</span>
<span id="509"> 509</span>
<span id="510"> 510</span>
<span id="511"> 511</span>
<span id="512"> 512</span>
<span id="513"> 513</span>
<span id="514"> 514</span>
<span id="515"> 515</span>
<span id="516"> 516</span>
<span id="517"> 517</span>
<span id="518"> 518</span>
<span id="519"> 519</span>
<span id="520"> 520</span>
<span id="521"> 521</span>
<span id="522"> 522</span>
<span id="523"> 523</span>
<span id="524"> 524</span>
<span id="525"> 525</span>
<span id="526"> 526</span>
<span id="527"> 527</span>
<span id="528"> 528</span>
<span id="529"> 529</span>
<span id="530"> 530</span>
<span id="531"> 531</span>
<span id="532"> 532</span>
<span id="533"> 533</span>
<span id="534"> 534</span>
<span id="535"> 535</span>
<span id="536"> 536</span>
<span id="537"> 537</span>
<span id="538"> 538</span>
<span id="539"> 539</span>
<span id="540"> 540</span>
<span id="541"> 541</span>
<span id="542"> 542</span>
<span id="543"> 543</span>
<span id="544"> 544</span>
<span id="545"> 545</span>
<span id="546"> 546</span>
<span id="547"> 547</span>
<span id="548"> 548</span>
<span id="549"> 549</span>
<span id="550"> 550</span>
<span id="551"> 551</span>
<span id="552"> 552</span>
<span id="553"> 553</span>
<span id="554"> 554</span>
<span id="555"> 555</span>
<span id="556"> 556</span>
<span id="557"> 557</span>
<span id="558"> 558</span>
<span id="559"> 559</span>
<span id="560"> 560</span>
<span id="561"> 561</span>
<span id="562"> 562</span>
<span id="563"> 563</span>
<span id="564"> 564</span>
<span id="565"> 565</span>
<span id="566"> 566</span>
<span id="567"> 567</span>
<span id="568"> 568</span>
<span id="569"> 569</span>
<span id="570"> 570</span>
<span id="571"> 571</span>
<span id="572"> 572</span>
<span id="573"> 573</span>
<span id="574"> 574</span>
<span id="575"> 575</span>
<span id="576"> 576</span>
<span id="577"> 577</span>
<span id="578"> 578</span>
<span id="579"> 579</span>
<span id="580"> 580</span>
<span id="581"> 581</span>
<span id="582"> 582</span>
<span id="583"> 583</span>
<span id="584"> 584</span>
<span id="585"> 585</span>
<span id="586"> 586</span>
<span id="587"> 587</span>
<span id="588"> 588</span>
<span id="589"> 589</span>
<span id="590"> 590</span>
<span id="591"> 591</span>
<span id="592"> 592</span>
<span id="593"> 593</span>
<span id="594"> 594</span>
<span id="595"> 595</span>
<span id="596"> 596</span>
<span id="597"> 597</span>
<span id="598"> 598</span>
<span id="599"> 599</span>
<span id="600"> 600</span>
<span id="601"> 601</span>
<span id="602"> 602</span>
<span id="603"> 603</span>
<span id="604"> 604</span>
<span id="605"> 605</span>
<span id="606"> 606</span>
<span id="607"> 607</span>
<span id="608"> 608</span>
<span id="609"> 609</span>
<span id="610"> 610</span>
<span id="611"> 611</span>
<span id="612"> 612</span>
<span id="613"> 613</span>
<span id="614"> 614</span>
<span id="615"> 615</span>
<span id="616"> 616</span>
<span id="617"> 617</span>
<span id="618"> 618</span>
<span id="619"> 619</span>
<span id="620"> 620</span>
<span id="621"> 621</span>
<span id="622"> 622</span>
<span id="623"> 623</span>
<span id="624"> 624</span>
<span id="625"> 625</span>
<span id="626"> 626</span>
<span id="627"> 627</span>
<span id="628"> 628</span>
<span id="629"> 629</span>
<span id="630"> 630</span>
<span id="631"> 631</span>
<span id="632"> 632</span>
<span id="633"> 633</span>
<span id="634"> 634</span>
<span id="635"> 635</span>
<span id="636"> 636</span>
<span id="637"> 637</span>
<span id="638"> 638</span>
<span id="639"> 639</span>
<span id="640"> 640</span>
<span id="641"> 641</span>
<span id="642"> 642</span>
<span id="643"> 643</span>
<span id="644"> 644</span>
<span id="645"> 645</span>
<span id="646"> 646</span>
<span id="647"> 647</span>
<span id="648"> 648</span>
<span id="649"> 649</span>
<span id="650"> 650</span>
<span id="651"> 651</span>
<span id="652"> 652</span>
<span id="653"> 653</span>
<span id="654"> 654</span>
<span id="655"> 655</span>
<span id="656"> 656</span>
<span id="657"> 657</span>
<span id="658"> 658</span>
<span id="659"> 659</span>
<span id="660"> 660</span>
<span id="661"> 661</span>
<span id="662"> 662</span>
<span id="663"> 663</span>
<span id="664"> 664</span>
<span id="665"> 665</span>
<span id="666"> 666</span>
<span id="667"> 667</span>
<span id="668"> 668</span>
<span id="669"> 669</span>
<span id="670"> 670</span>
<span id="671"> 671</span>
<span id="672"> 672</span>
<span id="673"> 673</span>
<span id="674"> 674</span>
<span id="675"> 675</span>
<span id="676"> 676</span>
<span id="677"> 677</span>
<span id="678"> 678</span>
<span id="679"> 679</span>
<span id="680"> 680</span>
<span id="681"> 681</span>
<span id="682"> 682</span>
<span id="683"> 683</span>
<span id="684"> 684</span>
<span id="685"> 685</span>
<span id="686"> 686</span>
<span id="687"> 687</span>
<span id="688"> 688</span>
<span id="689"> 689</span>
<span id="690"> 690</span>
<span id="691"> 691</span>
<span id="692"> 692</span>
<span id="693"> 693</span>
<span id="694"> 694</span>
<span id="695"> 695</span>
<span id="696"> 696</span>
<span id="697"> 697</span>
<span id="698"> 698</span>
<span id="699"> 699</span>
<span id="700"> 700</span>
<span id="701"> 701</span>
<span id="702"> 702</span>
<span id="703"> 703</span>
<span id="704"> 704</span>
<span id="705"> 705</span>
<span id="706"> 706</span>
<span id="707"> 707</span>
<span id="708"> 708</span>
<span id="709"> 709</span>
<span id="710"> 710</span>
<span id="711"> 711</span>
<span id="712"> 712</span>
<span id="713"> 713</span>
<span id="714"> 714</span>
<span id="715"> 715</span>
<span id="716"> 716</span>
<span id="717"> 717</span>
<span id="718"> 718</span>
<span id="719"> 719</span>
<span id="720"> 720</span>
<span id="721"> 721</span>
<span id="722"> 722</span>
<span id="723"> 723</span>
<span id="724"> 724</span>
<span id="725"> 725</span>
<span id="726"> 726</span>
<span id="727"> 727</span>
<span id="728"> 728</span>
<span id="729"> 729</span>
<span id="730"> 730</span>
<span id="731"> 731</span>
<span id="732"> 732</span>
<span id="733"> 733</span>
<span id="734"> 734</span>
<span id="735"> 735</span>
<span id="736"> 736</span>
<span id="737"> 737</span>
<span id="738"> 738</span>
<span id="739"> 739</span>
<span id="740"> 740</span>
<span id="741"> 741</span>
<span id="742"> 742</span>
<span id="743"> 743</span>
<span id="744"> 744</span>
<span id="745"> 745</span>
<span id="746"> 746</span>
<span id="747"> 747</span>
<span id="748"> 748</span>
<span id="749"> 749</span>
<span id="750"> 750</span>
<span id="751"> 751</span>
<span id="752"> 752</span>
<span id="753"> 753</span>
<span id="754"> 754</span>
<span id="755"> 755</span>
<span id="756"> 756</span>
<span id="757"> 757</span>
<span id="758"> 758</span>
<span id="759"> 759</span>
<span id="760"> 760</span>
<span id="761"> 761</span>
<span id="762"> 762</span>
<span id="763"> 763</span>
<span id="764"> 764</span>
<span id="765"> 765</span>
<span id="766"> 766</span>
<span id="767"> 767</span>
<span id="768"> 768</span>
<span id="769"> 769</span>
<span id="770"> 770</span>
<span id="771"> 771</span>
<span id="772"> 772</span>
<span id="773"> 773</span>
<span id="774"> 774</span>
<span id="775"> 775</span>
<span id="776"> 776</span>
<span id="777"> 777</span>
<span id="778"> 778</span>
<span id="779"> 779</span>
<span id="780"> 780</span>
<span id="781"> 781</span>
<span id="782"> 782</span>
<span id="783"> 783</span>
<span id="784"> 784</span>
<span id="785"> 785</span>
<span id="786"> 786</span>
<span id="787"> 787</span>
<span id="788"> 788</span>
<span id="789"> 789</span>
<span id="790"> 790</span>
<span id="791"> 791</span>
<span id="792"> 792</span>
<span id="793"> 793</span>
<span id="794"> 794</span>
<span id="795"> 795</span>
<span id="796"> 796</span>
<span id="797"> 797</span>
<span id="798"> 798</span>
<span id="799"> 799</span>
<span id="800"> 800</span>
<span id="801"> 801</span>
<span id="802"> 802</span>
<span id="803"> 803</span>
<span id="804"> 804</span>
<span id="805"> 805</span>
<span id="806"> 806</span>
<span id="807"> 807</span>
<span id="808"> 808</span>
<span id="809"> 809</span>
<span id="810"> 810</span>
<span id="811"> 811</span>
<span id="812"> 812</span>
<span id="813"> 813</span>
<span id="814"> 814</span>
<span id="815"> 815</span>
<span id="816"> 816</span>
<span id="817"> 817</span>
<span id="818"> 818</span>
<span id="819"> 819</span>
<span id="820"> 820</span>
<span id="821"> 821</span>
<span id="822"> 822</span>
<span id="823"> 823</span>
<span id="824"> 824</span>
<span id="825"> 825</span>
<span id="826"> 826</span>
<span id="827"> 827</span>
<span id="828"> 828</span>
<span id="829"> 829</span>
<span id="830"> 830</span>
<span id="831"> 831</span>
<span id="832"> 832</span>
<span id="833"> 833</span>
<span id="834"> 834</span>
<span id="835"> 835</span>
<span id="836"> 836</span>
<span id="837"> 837</span>
<span id="838"> 838</span>
<span id="839"> 839</span>
<span id="840"> 840</span>
<span id="841"> 841</span>
<span id="842"> 842</span>
<span id="843"> 843</span>
<span id="844"> 844</span>
<span id="845"> 845</span>
<span id="846"> 846</span>
<span id="847"> 847</span>
<span id="848"> 848</span>
<span id="849"> 849</span>
<span id="850"> 850</span>
<span id="851"> 851</span>
<span id="852"> 852</span>
<span id="853"> 853</span>
<span id="854"> 854</span>
<span id="855"> 855</span>
<span id="856"> 856</span>
<span id="857"> 857</span>
<span id="858"> 858</span>
<span id="859"> 859</span>
<span id="860"> 860</span>
<span id="861"> 861</span>
<span id="862"> 862</span>
<span id="863"> 863</span>
<span id="864"> 864</span>
<span id="865"> 865</span>
<span id="866"> 866</span>
<span id="867"> 867</span>
<span id="868"> 868</span>
<span id="869"> 869</span>
<span id="870"> 870</span>
<span id="871"> 871</span>
<span id="872"> 872</span>
<span id="873"> 873</span>
<span id="874"> 874</span>
<span id="875"> 875</span>
<span id="876"> 876</span>
<span id="877"> 877</span>
<span id="878"> 878</span>
<span id="879"> 879</span>
<span id="880"> 880</span>
<span id="881"> 881</span>
<span id="882"> 882</span>
<span id="883"> 883</span>
<span id="884"> 884</span>
<span id="885"> 885</span>
<span id="886"> 886</span>
<span id="887"> 887</span>
<span id="888"> 888</span>
<span id="889"> 889</span>
<span id="890"> 890</span>
<span id="891"> 891</span>
<span id="892"> 892</span>
<span id="893"> 893</span>
<span id="894"> 894</span>
<span id="895"> 895</span>
<span id="896"> 896</span>
<span id="897"> 897</span>
<span id="898"> 898</span>
<span id="899"> 899</span>
<span id="900"> 900</span>
<span id="901"> 901</span>
<span id="902"> 902</span>
<span id="903"> 903</span>
<span id="904"> 904</span>
<span id="905"> 905</span>
<span id="906"> 906</span>
<span id="907"> 907</span>
<span id="908"> 908</span>
<span id="909"> 909</span>
<span id="910"> 910</span>
<span id="911"> 911</span>
<span id="912"> 912</span>
<span id="913"> 913</span>
<span id="914"> 914</span>
<span id="915"> 915</span>
<span id="916"> 916</span>
<span id="917"> 917</span>
<span id="918"> 918</span>
<span id="919"> 919</span>
<span id="920"> 920</span>
<span id="921"> 921</span>
<span id="922"> 922</span>
<span id="923"> 923</span>
<span id="924"> 924</span>
<span id="925"> 925</span>
<span id="926"> 926</span>
<span id="927"> 927</span>
<span id="928"> 928</span>
<span id="929"> 929</span>
<span id="930"> 930</span>
<span id="931"> 931</span>
<span id="932"> 932</span>
<span id="933"> 933</span>
<span id="934"> 934</span>
<span id="935"> 935</span>
<span id="936"> 936</span>
<span id="937"> 937</span>
<span id="938"> 938</span>
<span id="939"> 939</span>
<span id="940"> 940</span>
<span id="941"> 941</span>
<span id="942"> 942</span>
<span id="943"> 943</span>
<span id="944"> 944</span>
<span id="945"> 945</span>
<span id="946"> 946</span>
<span id="947"> 947</span>
<span id="948"> 948</span>
<span id="949"> 949</span>
<span id="950"> 950</span>
<span id="951"> 951</span>
<span id="952"> 952</span>
<span id="953"> 953</span>
<span id="954"> 954</span>
<span id="955"> 955</span>
<span id="956"> 956</span>
<span id="957"> 957</span>
<span id="958"> 958</span>
<span id="959"> 959</span>
<span id="960"> 960</span>
<span id="961"> 961</span>
<span id="962"> 962</span>
<span id="963"> 963</span>
<span id="964"> 964</span>
<span id="965"> 965</span>
<span id="966"> 966</span>
<span id="967"> 967</span>
<span id="968"> 968</span>
<span id="969"> 969</span>
<span id="970"> 970</span>
<span id="971"> 971</span>
<span id="972"> 972</span>
<span id="973"> 973</span>
<span id="974"> 974</span>
<span id="975"> 975</span>
<span id="976"> 976</span>
<span id="977"> 977</span>
<span id="978"> 978</span>
<span id="979"> 979</span>
<span id="980"> 980</span>
<span id="981"> 981</span>
<span id="982"> 982</span>
<span id="983"> 983</span>
<span id="984"> 984</span>
<span id="985"> 985</span>
<span id="986"> 986</span>
<span id="987"> 987</span>
<span id="988"> 988</span>
<span id="989"> 989</span>
<span id="990"> 990</span>
<span id="991"> 991</span>
<span id="992"> 992</span>
<span id="993"> 993</span>
<span id="994"> 994</span>
<span id="995"> 995</span>
<span id="996"> 996</span>
<span id="997"> 997</span>
<span id="998"> 998</span>
<span id="999"> 999</span>
<span id="1000">1000</span>
<span id="1001">1001</span>
<span id="1002">1002</span>
<span id="1003">1003</span>
<span id="1004">1004</span>
<span id="1005">1005</span>
<span id="1006">1006</span>
<span id="1007">1007</span>
<span id="1008">1008</span>
<span id="1009">1009</span>
<span id="1010">1010</span>
<span id="1011">1011</span>
<span id="1012">1012</span>
<span id="1013">1013</span>
<span id="1014">1014</span>
<span id="1015">1015</span>
<span id="1016">1016</span>
<span id="1017">1017</span>
<span id="1018">1018</span>
<span id="1019">1019</span>
<span id="1020">1020</span>
<span id="1021">1021</span>
<span id="1022">1022</span>
<span id="1023">1023</span>
<span id="1024">1024</span>
<span id="1025">1025</span>
<span id="1026">1026</span>
<span id="1027">1027</span>
<span id="1028">1028</span>
<span id="1029">1029</span>
<span id="1030">1030</span>
<span id="1031">1031</span>
</pre><pre class='rust '>
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>libc</span>;
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>fann_sys</span>;

<span class='kw'>use</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_activationfunc_enum</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_errorfunc_enum</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_nettype_enum</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_stopfunc_enum</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_train_enum</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_type</span>;
<span class='kw'>use</span> <span class='ident'>libc</span>::{<span class='ident'>c_float</span>, <span class='ident'>c_int</span>, <span class='ident'>c_uint</span>};
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>ffi</span>::<span class='ident'>CString</span>;
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>path</span>::<span class='ident'>Path</span>;
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>ptr</span>::<span class='ident'>copy_nonoverlapping</span>;

<span class='kw'>pub</span> <span class='kw'>use</span> <span class='ident'>error</span>::{<span class='ident'>FannError</span>, <span class='ident'>FannErrorType</span>, <span class='ident'>FannResult</span>};
<span class='kw'>pub</span> <span class='kw'>use</span> <span class='ident'>train_data</span>::<span class='ident'>TrainData</span>;

<span class='kw'>mod</span> <span class='ident'>error</span>;
<span class='kw'>mod</span> <span class='ident'>train_data</span>;

<span class='kw'>pub</span> <span class='kw'>type</span> <span class='ident'>Connection</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_connection</span>;

<span class='doccomment'>/// Convert a path to a `CString`.</span>
<span class='kw'>fn</span> <span class='ident'>to_filename</span><span class='op'>&lt;</span><span class='ident'>P</span>: <span class='ident'>AsRef</span><span class='op'>&lt;</span><span class='ident'>Path</span><span class='op'>&gt;&gt;</span>(<span class='ident'>path</span>: <span class='ident'>P</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span><span class='ident'>CString</span>, <span class='ident'>FannError</span><span class='op'>&gt;</span> {
    <span class='kw'>match</span> <span class='ident'>path</span>.<span class='ident'>as_ref</span>().<span class='ident'>to_str</span>().<span class='ident'>map</span>(<span class='op'>|</span><span class='ident'>s</span><span class='op'>|</span> <span class='ident'>CString</span>::<span class='ident'>new</span>(<span class='ident'>s</span>)) {
        <span class='prelude-val'>None</span> <span class='op'>=&gt;</span> <span class='prelude-val'>Err</span>(<span class='ident'>FannError</span> {
                    <span class='ident'>error_type</span>: <span class='ident'>FannErrorType</span>::<span class='ident'>CantOpenTdR</span>,
                    <span class='ident'>error_str</span>: <span class='string'>&quot;File name contains invalid unicode characters&quot;</span>.<span class='ident'>to_string</span>(),
                }),
        <span class='prelude-val'>Some</span>(<span class='prelude-val'>Err</span>(<span class='ident'>e</span>)) <span class='op'>=&gt;</span> <span class='prelude-val'>Err</span>(<span class='ident'>FannError</span> {
                            <span class='ident'>error_type</span>: <span class='ident'>FannErrorType</span>::<span class='ident'>CantOpenTdR</span>,
                            <span class='ident'>error_str</span>: <span class='macro'>format</span><span class='macro'>!</span>(<span class='string'>&quot;File name contains a nul byte at position {}&quot;</span>,
                                               <span class='ident'>e</span>.<span class='ident'>nul_position</span>()),
                        }),
        <span class='prelude-val'>Some</span>(<span class='prelude-val'>Ok</span>(<span class='ident'>cs</span>)) <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>cs</span>),
    }
}

<span class='doccomment'>/// The Training algorithms used when training on `fann_train_data` with functions like</span>
<span class='doccomment'>/// `fann_train_on_data` or `fann_train_on_file`. The incremental training alters the weights</span>
<span class='doccomment'>/// after each time it is presented an input pattern, while batch only alters the weights once after</span>
<span class='doccomment'>/// it has been presented to all the patterns.</span>
<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Copy</span>, <span class='ident'>Clone</span>, <span class='ident'>Debug</span>, <span class='ident'>PartialEq</span>)]</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>TrainAlgorithm</span> {
    <span class='doccomment'>/// Standard backpropagation algorithm, where the weights are updated after each training</span>
    <span class='doccomment'>/// pattern. This means that the weights are updated many times during a single epoch and some</span>
    <span class='doccomment'>/// problems will train very fast, while other more advanced problems will not train very well.</span>
    <span class='ident'>Incremental</span>,
    <span class='doccomment'>/// Standard backpropagation algorithm, where the weights are updated after calculating the mean</span>
    <span class='doccomment'>/// square error for the whole training set. This means that the weights are only updated once</span>
    <span class='doccomment'>/// during an epoch. For this reason some problems will train slower with this algorithm. But</span>
    <span class='doccomment'>/// since the mean square error is calculated more correctly than in incremental training, some</span>
    <span class='doccomment'>/// problems will reach better solutions.</span>
    <span class='ident'>Batch</span>,
    <span class='doccomment'>/// A more advanced batch training algorithm which achieves good results for many problems.</span>
    <span class='doccomment'>/// `Rprop` is adaptive and therefore does not use the `learning_rate`. Some other parameters</span>
    <span class='doccomment'>/// can, however, be set to change the way `Rprop` works, but it is only recommended for users</span>
    <span class='doccomment'>/// with a deep understanding of the algorithm. The original RPROP training algorithm is</span>
    <span class='doccomment'>/// described by [Riedmiller and Braun, 1993], but the algorithm used here is a variant, iRPROP,</span>
    <span class='doccomment'>/// described by [Igel and Husken, 2000].</span>
    <span class='ident'>Rprop</span> {
        <span class='doccomment'>/// A value less than 1, used to decrease the step size during training. Default 0.5</span>
        <span class='ident'>decrease_factor</span>: <span class='ident'>c_float</span>,
        <span class='doccomment'>/// A value greater than 1, used to increase the step size during training. Default 1.2</span>
        <span class='ident'>increase_factor</span>: <span class='ident'>c_float</span>,
        <span class='doccomment'>/// The minimum step size. Default 0.0</span>
        <span class='ident'>delta_min</span>: <span class='ident'>c_float</span>,
        <span class='doccomment'>/// The maximum step size. Default 50.0</span>
        <span class='ident'>delta_max</span>: <span class='ident'>c_float</span>,
        <span class='doccomment'>/// The initial step size. Default 0.1</span>
        <span class='ident'>delta_zero</span>: <span class='ident'>c_float</span>,
    },
    <span class='doccomment'>/// A more advanced batch training algorithm which achieves good results for many problems. The</span>
    <span class='doccomment'>/// quickprop training algorithm uses the `learning_rate` parameter along with other more</span>
    <span class='doccomment'>/// advanced parameters, but it is only recommended to change these for users with a deep</span>
    <span class='doccomment'>/// understanding of the algorithm. Quickprop is described by [Fahlman, 1988].</span>
    <span class='ident'>Quickprop</span> {
        <span class='doccomment'>/// The factor by which weights should become smaller in each iteration, to ensure that</span>
        <span class='doccomment'>/// the weights don&#39;t grow too large during training. Should be a negative number close to</span>
        <span class='doccomment'>/// 0. The default is -0.0001.</span>
        <span class='ident'>decay</span>: <span class='ident'>c_float</span>,
        <span class='doccomment'>/// The mu factor is used to increase or decrease the step size; should always be greater</span>
        <span class='doccomment'>/// than 1. The default is 1.75.</span>
        <span class='ident'>mu</span>: <span class='ident'>c_float</span>,
    },
}

<span class='kw'>impl</span> <span class='ident'>TrainAlgorithm</span> {
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>default_rprop</span>() <span class='op'>-&gt;</span> <span class='ident'>TrainAlgorithm</span> {
        <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Rprop</span> {
            <span class='ident'>decrease_factor</span>: <span class='number'>0.5</span>,
            <span class='ident'>increase_factor</span>: <span class='number'>1.2</span>,
            <span class='ident'>delta_min</span>: <span class='number'>0.0</span>,
            <span class='ident'>delta_max</span>: <span class='number'>50.0</span>,
            <span class='ident'>delta_zero</span>: <span class='number'>0.1</span>,
        }
    }
    
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>default_quickprop</span>() <span class='op'>-&gt;</span> <span class='ident'>TrainAlgorithm</span> {
        <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Quickprop</span> {
            <span class='ident'>decay</span>: <span class='op'>-</span><span class='number'>0.0001</span>,
            <span class='ident'>mu</span>: <span class='number'>1.75</span>,
        }
    }
}

<span class='kw'>impl</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>TrainAlgorithm</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>TrainAlgorithm</span> {
        <span class='ident'>TrainAlgorithm</span>::<span class='ident'>default_rprop</span>()
    }
}

<span class='doccomment'>/// The activation functions used for the neurons during training. They can either be set for a</span>
<span class='doccomment'>/// group of neurons using `set_activation_func_hidden` and `set_activation_func_output`, or for a</span>
<span class='doccomment'>/// single neuron using `set_activation_func`.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// Similarly, the steepness of an activation function is specified using</span>
<span class='doccomment'>/// `set_activation_steepness_hidden`, `set_activation_steepness_output` and</span>
<span class='doccomment'>/// `set_activation_steepness`.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// In the descriptions of the functions:</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// * x is the input to the activation function,</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// * y is the output,</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// * s is the steepness and</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// * d is the derivation.</span>
<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Copy</span>, <span class='ident'>Clone</span>, <span class='ident'>Debug</span>, <span class='ident'>Eq</span>, <span class='ident'>PartialEq</span>)]</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>ActivationFunc</span> {
    <span class='doccomment'>/// Linear activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: -inf &lt; y &lt; inf</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = x*s, d = 1*s</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * Can NOT be used in fixed point.</span>
    <span class='ident'>Linear</span>,
    <span class='doccomment'>/// Threshold activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * x &lt; 0 -&gt; y = 0, x &gt;= 0 -&gt; y = 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * Can NOT be used during training.</span>
    <span class='ident'>Threshold</span>,
    <span class='doccomment'>/// Threshold activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * x &lt; 0 -&gt; y = 0, x &gt;= 0 -&gt; y = 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * Can NOT be used during training.</span>
    <span class='ident'>ThresholdSymmetric</span>,
    <span class='doccomment'>/// Sigmoid activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * One of the most used activation functions.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: 0 &lt; y &lt; 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = 1/(1 + exp(-2*s*x))</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = 2*s*y*(1 - y)</span>
    <span class='ident'>Sigmoid</span>,
    <span class='doccomment'>/// Stepwise linear approximation to sigmoid.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * Faster than sigmoid but a bit less precise.</span>
    <span class='ident'>SigmoidStepwise</span>,
    <span class='doccomment'>/// Symmetric sigmoid activation function, aka. tanh.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * One of the most used activation functions.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: -1 &lt; y &lt; 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = tanh(s*x) = 2/(1 + exp(-2*s*x)) - 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = s*(1-(y*y))</span>
    <span class='ident'>SigmoidSymmetric</span>,
    <span class='doccomment'>/// Stepwise linear approximation to symmetric sigmoid.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * Faster than symmetric sigmoid but a bit less precise.</span>
    <span class='ident'>SigmoidSymmetricStepwise</span>,
    <span class='doccomment'>/// Gaussian activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * 0 when x = -inf, 1 when x = 0 and 0 when x = inf</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: 0 &lt; y &lt; 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = exp(-x*s*x*s)</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = -2*x*s*y*s</span>
    <span class='ident'>Gaussian</span>,
    <span class='doccomment'>/// Symmetric gaussian activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * -1 when x = -inf, 1 when x = 0 and 0 when x = inf</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: -1 &lt; y &lt; 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = exp(-x*s*x*s)*2-1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = -2*x*s*(y+1)*s</span>
    <span class='ident'>GaussianSymmetric</span>,
    <span class='doccomment'>/// Stepwise linear approximation to gaussian.</span>
    <span class='doccomment'>/// Faster than gaussian but a bit less precise.</span>
    <span class='doccomment'>/// NOT implemented yet.</span>
    <span class='ident'>GaussianStepwise</span>,
    <span class='doccomment'>/// Fast (sigmoid like) activation function defined by David Elliott</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: 0 &lt; y &lt; 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = ((x*s) / 2) / (1 + |x*s|) + 0.5</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = s*1/(2*(1+|x*s|)*(1+|x*s|))</span>
    <span class='ident'>Elliott</span>,
    <span class='doccomment'>/// Fast (symmetric sigmoid like) activation function defined by David Elliott</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: -1 &lt; y &lt; 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = (x*s) / (1 + |x*s|)</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = s*1/((1+|x*s|)*(1+|x*s|))</span>
    <span class='ident'>ElliottSymmetric</span>,
    <span class='doccomment'>/// Bounded linear activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: 0 &lt;= y &lt;= 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = x*s, d = 1*s</span>
    <span class='ident'>LinearPiece</span>,
    <span class='doccomment'>/// Bounded linear activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: -1 &lt;= y &lt;= 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = x*s, d = 1*s</span>
    <span class='ident'>LinearPieceSymmetric</span>,
    <span class='doccomment'>/// Periodical sine activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: -1 &lt;= y &lt;= 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = sin(x*s)</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = s*cos(x*s)</span>
    <span class='ident'>SinSymmetric</span>,
    <span class='doccomment'>/// Periodical cosine activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: -1 &lt;= y &lt;= 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = cos(x*s)</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = s*-sin(x*s)</span>
    <span class='ident'>CosSymmetric</span>,
    <span class='doccomment'>/// Periodical sine activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: 0 &lt;= y &lt;= 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = sin(x*s)/2+0.5</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = s*cos(x*s)/2</span>
    <span class='ident'>Sin</span>,
    <span class='doccomment'>/// Periodical cosine activation function.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * span: 0 &lt;= y &lt;= 1</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * y = cos(x*s)/2+0.5</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * d = s*-sin(x*s)/2</span>
    <span class='ident'>Cos</span>,
}

<span class='kw'>impl</span> <span class='ident'>ActivationFunc</span> {
    <span class='kw'>fn</span> <span class='ident'>from_fann_activationfunc_enum</span>(<span class='ident'>af_enum</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_activationfunc_enum</span>)
            <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>ActivationFunc</span><span class='op'>&gt;</span> {
        <span class='kw'>match</span> <span class='ident'>af_enum</span> {
            <span class='ident'>FANN_NONE</span> <span class='op'>=&gt;</span> <span class='prelude-val'>Err</span>(<span class='ident'>FannError</span> {
                             <span class='ident'>error_type</span>: <span class='ident'>FannErrorType</span>::<span class='ident'>IndexOutOfBound</span>,
                             <span class='ident'>error_str</span>: <span class='string'>&quot;Neuron or layer index is out of bound.&quot;</span>.<span class='ident'>to_string</span>(),
                         }),
            <span class='ident'>FANN_LINEAR</span>                     <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Linear</span>),
            <span class='ident'>FANN_THRESHOLD</span>                  <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Threshold</span>),
            <span class='ident'>FANN_THRESHOLD_SYMMETRIC</span>        <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>ThresholdSymmetric</span>),
            <span class='ident'>FANN_SIGMOID</span>                    <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Sigmoid</span>),
            <span class='ident'>FANN_SIGMOID_STEPWISE</span>           <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidStepwise</span>),
            <span class='ident'>FANN_SIGMOID_SYMMETRIC</span>          <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidSymmetric</span>),
            <span class='ident'>FANN_SIGMOID_SYMMETRIC_STEPWISE</span> <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidSymmetricStepwise</span>),
            <span class='ident'>FANN_GAUSSIAN</span>                   <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Gaussian</span>),
            <span class='ident'>FANN_GAUSSIAN_SYMMETRIC</span>         <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>GaussianSymmetric</span>),
            <span class='ident'>FANN_GAUSSIAN_STEPWISE</span>          <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>GaussianStepwise</span>),
            <span class='ident'>FANN_ELLIOTT</span>                    <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Elliott</span>),
            <span class='ident'>FANN_ELLIOTT_SYMMETRIC</span>          <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>ElliottSymmetric</span>),
            <span class='ident'>FANN_LINEAR_PIECE</span>               <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>LinearPiece</span>),
            <span class='ident'>FANN_LINEAR_PIECE_SYMMETRIC</span>     <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>LinearPieceSymmetric</span>),
            <span class='ident'>FANN_SIN_SYMMETRIC</span>              <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>SinSymmetric</span>),
            <span class='ident'>FANN_COS_SYMMETRIC</span>              <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>CosSymmetric</span>),
            <span class='ident'>FANN_SIN</span>                        <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Sin</span>),
            <span class='ident'>FANN_COS</span>                        <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Cos</span>),
        }
    }

    <span class='kw'>fn</span> <span class='ident'>to_fann_activationfunc_enum</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_activationfunc_enum</span> {
        <span class='kw'>match</span> <span class='op'>*</span><span class='self'>self</span> {
            <span class='ident'>ActivationFunc</span>::<span class='ident'>Linear</span>                   <span class='op'>=&gt;</span> <span class='ident'>FANN_LINEAR</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>Threshold</span>                <span class='op'>=&gt;</span> <span class='ident'>FANN_THRESHOLD</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>ThresholdSymmetric</span>       <span class='op'>=&gt;</span> <span class='ident'>FANN_THRESHOLD_SYMMETRIC</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>Sigmoid</span>                  <span class='op'>=&gt;</span> <span class='ident'>FANN_SIGMOID</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidStepwise</span>          <span class='op'>=&gt;</span> <span class='ident'>FANN_SIGMOID_STEPWISE</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidSymmetric</span>         <span class='op'>=&gt;</span> <span class='ident'>FANN_SIGMOID_SYMMETRIC</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidSymmetricStepwise</span> <span class='op'>=&gt;</span> <span class='ident'>FANN_SIGMOID_SYMMETRIC_STEPWISE</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>Gaussian</span>                 <span class='op'>=&gt;</span> <span class='ident'>FANN_GAUSSIAN</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>GaussianSymmetric</span>        <span class='op'>=&gt;</span> <span class='ident'>FANN_GAUSSIAN_SYMMETRIC</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>GaussianStepwise</span>         <span class='op'>=&gt;</span> <span class='ident'>FANN_GAUSSIAN_STEPWISE</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>Elliott</span>                  <span class='op'>=&gt;</span> <span class='ident'>FANN_ELLIOTT</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>ElliottSymmetric</span>         <span class='op'>=&gt;</span> <span class='ident'>FANN_ELLIOTT_SYMMETRIC</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>LinearPiece</span>              <span class='op'>=&gt;</span> <span class='ident'>FANN_LINEAR_PIECE</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>LinearPieceSymmetric</span>     <span class='op'>=&gt;</span> <span class='ident'>FANN_LINEAR_PIECE_SYMMETRIC</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>SinSymmetric</span>             <span class='op'>=&gt;</span> <span class='ident'>FANN_SIN_SYMMETRIC</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>CosSymmetric</span>             <span class='op'>=&gt;</span> <span class='ident'>FANN_COS_SYMMETRIC</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>Sin</span>                      <span class='op'>=&gt;</span> <span class='ident'>FANN_SIN</span>,
            <span class='ident'>ActivationFunc</span>::<span class='ident'>Cos</span>                      <span class='op'>=&gt;</span> <span class='ident'>FANN_COS</span>,
        }
    }
}

<span class='doccomment'>/// Error function used during training.</span>
<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Copy</span>, <span class='ident'>Clone</span>, <span class='ident'>Eq</span>, <span class='ident'>PartialEq</span>)]</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>ErrorFunc</span> {
    <span class='doccomment'>/// Standard linear error function</span>
    <span class='ident'>Linear</span>,
    <span class='doccomment'>/// Tanh error function; usually better but may require a lower learning rate. This error</span>
    <span class='doccomment'>/// function aggressively targets outputs that differ much from the desired, while not targeting</span>
    <span class='doccomment'>/// outputs that only differ slightly. Not recommended for cascade or incremental training.</span>
    <span class='ident'>Tanh</span>,
}

<span class='kw'>impl</span> <span class='ident'>ErrorFunc</span> {
    <span class='kw'>fn</span> <span class='ident'>from_errorfunc_enum</span>(<span class='ident'>ef_enum</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_errorfunc_enum</span>) <span class='op'>-&gt;</span> <span class='ident'>ErrorFunc</span> {
        <span class='kw'>match</span> <span class='ident'>ef_enum</span> {
            <span class='ident'>FANN_ERRORFUNC_LINEAR</span> <span class='op'>=&gt;</span> <span class='ident'>ErrorFunc</span>::<span class='ident'>Linear</span>,
            <span class='ident'>FANN_ERRORFUNC_TANH</span>   <span class='op'>=&gt;</span> <span class='ident'>ErrorFunc</span>::<span class='ident'>Tanh</span>,
        }
    }

    <span class='kw'>fn</span> <span class='ident'>to_errorfunc_enum</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_errorfunc_enum</span> {
        <span class='kw'>match</span> <span class='op'>*</span><span class='self'>self</span> {
            <span class='ident'>ErrorFunc</span>::<span class='ident'>Linear</span> <span class='op'>=&gt;</span> <span class='ident'>FANN_ERRORFUNC_LINEAR</span>,
            <span class='ident'>ErrorFunc</span>::<span class='ident'>Tanh</span>   <span class='op'>=&gt;</span> <span class='ident'>FANN_ERRORFUNC_TANH</span>,
        }
    }
}

<span class='doccomment'>/// Stop critieria for training.</span>
<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Copy</span>, <span class='ident'>Clone</span>, <span class='ident'>Eq</span>, <span class='ident'>PartialEq</span>)]</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>StopFunc</span> {
    <span class='doccomment'>/// The mean square error of the whole output.</span>
    <span class='ident'>Mse</span>,
    <span class='doccomment'>/// The number of training data points where the output neuron&#39;s error was greater than the bit</span>
    <span class='doccomment'>/// fail limit. Every neuron is counted for every training data sample where it fails.</span>
    <span class='ident'>Bit</span>,
}

<span class='kw'>impl</span> <span class='ident'>StopFunc</span> {
    <span class='kw'>fn</span> <span class='ident'>from_stopfunc_enum</span>(<span class='ident'>sf_enum</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_stopfunc_enum</span>) <span class='op'>-&gt;</span> <span class='ident'>StopFunc</span> {
        <span class='kw'>match</span> <span class='ident'>sf_enum</span> {
            <span class='ident'>FANN_STOPFUNC_MSE</span> <span class='op'>=&gt;</span> <span class='ident'>StopFunc</span>::<span class='ident'>Mse</span>,
            <span class='ident'>FANN_STOPFUNC_BIT</span> <span class='op'>=&gt;</span> <span class='ident'>StopFunc</span>::<span class='ident'>Bit</span>,
        }
    }

    <span class='kw'>fn</span> <span class='ident'>to_stopfunc_enum</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_stopfunc_enum</span> {
        <span class='kw'>match</span> <span class='op'>*</span><span class='self'>self</span> {
            <span class='ident'>StopFunc</span>::<span class='ident'>Mse</span> <span class='op'>=&gt;</span> <span class='ident'>FANN_STOPFUNC_MSE</span>,
            <span class='ident'>StopFunc</span>::<span class='ident'>Bit</span> <span class='op'>=&gt;</span> <span class='ident'>FANN_STOPFUNC_BIT</span>,
        }
    }
}

<span class='doccomment'>/// Network types</span>
<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Copy</span>, <span class='ident'>Clone</span>, <span class='ident'>Debug</span>, <span class='ident'>Eq</span>, <span class='ident'>PartialEq</span>)]</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>NetType</span> {
    <span class='doccomment'>/// Each layer of neurons only has connections to the next layer.</span>
    <span class='ident'>Layer</span>,
    <span class='doccomment'>/// Each layer has connections to all following layers.</span>
    <span class='ident'>Shortcut</span>,
}

<span class='kw'>impl</span> <span class='ident'>NetType</span> {
    <span class='kw'>fn</span> <span class='ident'>from_nettype_enum</span>(<span class='ident'>nt_enum</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_nettype_enum</span>) <span class='op'>-&gt;</span> <span class='ident'>NetType</span> {
        <span class='kw'>match</span> <span class='ident'>nt_enum</span> {
            <span class='ident'>FANN_NETTYPE_LAYER</span>    <span class='op'>=&gt;</span> <span class='ident'>NetType</span>::<span class='ident'>Layer</span>,
            <span class='ident'>FANN_NETTYPE_SHORTCUT</span> <span class='op'>=&gt;</span> <span class='ident'>NetType</span>::<span class='ident'>Shortcut</span>,
        }
    }
}
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>Fann</span> {
    <span class='comment'>// We don&#39;t consider setting and clearing the error string and number a mutation, and every</span>
    <span class='comment'>// method should leave these fields cleared, either because it succeeded or because it read the</span>
    <span class='comment'>// fields and returned the corresponding error.</span>
    <span class='comment'>// We also don&#39;t consider writing the output data a mutation, as we don&#39;t provide access to it</span>
    <span class='comment'>// and copy it before returning it.</span>
    <span class='ident'>raw</span>: <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann</span>,
}

<span class='kw'>impl</span> <span class='ident'>Fann</span> {
    <span class='doccomment'>/// Create a fully connected neural network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// There will be a bias neuron in each layer except the output layer,</span>
    <span class='doccomment'>/// and this bias neuron will be connected to all neurons in the next layer.</span>
    <span class='doccomment'>/// When running the network, the bias nodes always emit 1.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// # Arguments</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * `layers` - Specifies the number of neurons in each layer, starting with the input and</span>
    <span class='doccomment'>///              ending with the output layer.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// # Example</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// ```</span>
    <span class='doccomment'>/// // Creating a network with 2 input neurons, 1 output neuron,</span>
    <span class='doccomment'>/// // and two hidden layers with 8 and 9 neurons.</span>
    <span class='doccomment'>/// let layers = [2, 8, 9, 1];</span>
    <span class='doccomment'>/// fann::Fann::new(&amp;layers).unwrap();</span>
    <span class='doccomment'>/// ```</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>new</span>(<span class='ident'>layers</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>c_uint</span>]) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>Fann</span><span class='op'>&gt;</span> {
        <span class='ident'>Fann</span>::<span class='ident'>new_sparse</span>(<span class='number'>1.0</span>, <span class='ident'>layers</span>)
    }

    <span class='doccomment'>/// Create a neural network that is not necessarily fully connected.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// There will be a bias neuron in each layer except the output layer,</span>
    <span class='doccomment'>/// and this bias neuron will be connected to all neurons in the next layer.</span>
    <span class='doccomment'>/// When running the network, the bias nodes always emit 1.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// # Arguments</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * `connection_rate` - The share of pairs of neurons in consecutive layers that will be</span>
    <span class='doccomment'>///                       connected.</span>
    <span class='doccomment'>/// * `layers`          - Specifies the number of neurons in each layer, starting with the input</span>
    <span class='doccomment'>///                       and ending with the output layer.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>new_sparse</span>(<span class='ident'>connection_rate</span>: <span class='ident'>c_float</span>, <span class='ident'>layers</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>c_uint</span>]) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>Fann</span><span class='op'>&gt;</span> {
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>raw</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_create_sparse_array</span>(<span class='ident'>connection_rate</span>,
                                                         <span class='ident'>layers</span>.<span class='ident'>len</span>() <span class='kw'>as</span> <span class='ident'>c_uint</span>,
                                                         <span class='ident'>layers</span>.<span class='ident'>as_ptr</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='prelude-val'>Ok</span>(<span class='ident'>Fann</span> { <span class='ident'>raw</span>: <span class='ident'>raw</span> })
        }
    }

    <span class='doccomment'>/// Create a neural network which has shortcut connections, i. e. it doesn&#39;t connect only each</span>
    <span class='doccomment'>/// layer to its successor, but every layer with every later layer: Each neuron has connections</span>
    <span class='doccomment'>/// to all neurons in all subsequent layers.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>new_shortcut</span>(<span class='ident'>layers</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>c_uint</span>]) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>Fann</span><span class='op'>&gt;</span> {
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>raw</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_create_shortcut_array</span>(<span class='ident'>layers</span>.<span class='ident'>len</span>() <span class='kw'>as</span> <span class='ident'>c_uint</span>, <span class='ident'>layers</span>.<span class='ident'>as_ptr</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='prelude-val'>Ok</span>(<span class='ident'>Fann</span> { <span class='ident'>raw</span>: <span class='ident'>raw</span> })
        }
    }

    <span class='doccomment'>/// Read a neural network from a file.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>from_file</span><span class='op'>&lt;</span><span class='ident'>P</span>: <span class='ident'>AsRef</span><span class='op'>&lt;</span><span class='ident'>Path</span><span class='op'>&gt;&gt;</span>(<span class='ident'>path</span>: <span class='ident'>P</span>) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>Fann</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>filename</span> <span class='op'>=</span> <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>to_filename</span>(<span class='ident'>path</span>));
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>raw</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_create_from_file</span>(<span class='ident'>filename</span>.<span class='ident'>as_ptr</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='prelude-val'>Ok</span>(<span class='ident'>Fann</span> { <span class='ident'>raw</span>: <span class='ident'>raw</span> })
        }
    }

    <span class='doccomment'>/// Save the network to a configuration file.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The file will contain all information about the neural network, except parameters generated</span>
    <span class='doccomment'>/// during training, like mean square error and the bit fail limit.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>save</span><span class='op'>&lt;</span><span class='ident'>P</span>: <span class='ident'>AsRef</span><span class='op'>&lt;</span><span class='ident'>Path</span><span class='op'>&gt;&gt;</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>path</span>: <span class='ident'>P</span>) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span>()<span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>filename</span> <span class='op'>=</span> <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>to_filename</span>(<span class='ident'>path</span>));
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>result</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_save</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>filename</span>.<span class='ident'>as_ptr</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='kw'>match</span> <span class='ident'>result</span> {
                <span class='number'>0</span> <span class='op'>=&gt;</span> <span class='prelude-val'>Ok</span>(()),
                _ <span class='op'>=&gt;</span> <span class='prelude-val'>Err</span>(<span class='ident'>FannError</span> {
                         <span class='ident'>error_type</span>: <span class='ident'>FannErrorType</span>::<span class='ident'>CantSaveFile</span>,
                         <span class='ident'>error_str</span>: <span class='string'>&quot;Error saving network&quot;</span>.<span class='ident'>to_string</span>(),
                     }),
            }
        }
    }

    <span class='doccomment'>/// Give each connection a random weight between `min_weight` and `max_weight`.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// By default, weights in a new network are random between -0.1 and 0.1.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>randomize_weights</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>min_weight</span>: <span class='ident'>fann_type</span>, <span class='ident'>max_weight</span>: <span class='ident'>fann_type</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_randomize_weights</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>min_weight</span>, <span class='ident'>max_weight</span>) }
    }

    <span class='doccomment'>/// Initialize the weights using Widrow &amp; Nguyen&#39;s algorithm.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The algorithm developed by Derrick Nguyen and Bernard Widrow sets the weight in a way that</span>
    <span class='doccomment'>/// can speed up training with the given training data. This technique is not always successful</span>
    <span class='doccomment'>/// and in some cases can even be less efficient that a purely random initialization.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>init_weights</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>train_data</span>: <span class='kw-2'>&amp;</span><span class='ident'>TrainData</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_init_weights</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>train_data</span>.<span class='ident'>get_raw</span>()) }
    }

    <span class='doccomment'>/// Print the connections of the network in a compact matrix, for easy viewing of its</span>
    <span class='doccomment'>/// internals.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The output on a small (2 2 1) network trained on the xor problem:</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// ```text</span>
    <span class='doccomment'>/// Layer / Neuron 012345</span>
    <span class='doccomment'>/// L   1 / N    3 BBa...</span>
    <span class='doccomment'>/// L   1 / N    4 BBA...</span>
    <span class='doccomment'>/// L   1 / N    5 ......</span>
    <span class='doccomment'>/// L   2 / N    6 ...BBA</span>
    <span class='doccomment'>/// L   2 / N    7 ......</span>
    <span class='doccomment'>/// ```</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// This network has five real neurons and two bias neurons. This gives a total of seven</span>
    <span class='doccomment'>/// neurons named from 0 to 6. The connections between these neurons can be seen in the matrix.</span>
    <span class='doccomment'>/// &quot;.&quot; is a place where there is no connection, while a character tells how strong the</span>
    <span class='doccomment'>/// connection is on a scale from a-z. The two real neurons in the hidden layer (neuron 3 and 4</span>
    <span class='doccomment'>/// in layer 1) have connections from the three neurons in the previous layer as is visible in</span>
    <span class='doccomment'>/// the first two lines. The output neuron 6 has connections from the three neurons in the</span>
    <span class='doccomment'>/// hidden layer 3 - 5 as is visible in the fourth line.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// To simplify the matrix output neurons are not visible as neurons that connections can come</span>
    <span class='doccomment'>/// from, and input and bias neurons are not visible as neurons that connections can go to.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>print_connections</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_print_connections</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Print all parameters and options of the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>print_parameters</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_print_parameters</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Return an `Err` if the size of the slice does not match the number of input neurons,</span>
    <span class='doccomment'>/// otherwise `Ok(())`.</span>
    <span class='kw'>fn</span> <span class='ident'>check_input_size</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>input</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>fann_type</span>]) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span>()<span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>num_input</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>get_num_input</span>() <span class='kw'>as</span> <span class='ident'>usize</span>;
        <span class='kw'>if</span> <span class='ident'>input</span>.<span class='ident'>len</span>() <span class='op'>==</span> <span class='ident'>num_input</span> {
            <span class='prelude-val'>Ok</span>(())
        } <span class='kw'>else</span> {
            <span class='prelude-val'>Err</span>(<span class='ident'>FannError</span> {
                <span class='ident'>error_type</span>: <span class='ident'>FannErrorType</span>::<span class='ident'>IndexOutOfBound</span>,
                <span class='ident'>error_str</span>: <span class='macro'>format</span><span class='macro'>!</span>(<span class='string'>&quot;Input has length {}, but there are {} input neurons&quot;</span>,
                                   <span class='ident'>input</span>.<span class='ident'>len</span>(), <span class='ident'>num_input</span>),
            })
        }
    }

    <span class='doccomment'>/// Return an `Err` if the size of the slice does not match the number of output neurons,</span>
    <span class='doccomment'>/// otherwise `Ok(())`.</span>
    <span class='kw'>fn</span> <span class='ident'>check_output_size</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>output</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>fann_type</span>]) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span>()<span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>num_output</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>get_num_output</span>() <span class='kw'>as</span> <span class='ident'>usize</span>;
        <span class='kw'>if</span> <span class='ident'>output</span>.<span class='ident'>len</span>() <span class='op'>==</span> <span class='ident'>num_output</span> {
            <span class='prelude-val'>Ok</span>(())
        } <span class='kw'>else</span> {
            <span class='prelude-val'>Err</span>(<span class='ident'>FannError</span> {
                <span class='ident'>error_type</span>: <span class='ident'>FannErrorType</span>::<span class='ident'>IndexOutOfBound</span>,
                <span class='ident'>error_str</span>: <span class='macro'>format</span><span class='macro'>!</span>(<span class='string'>&quot;Output has length {}, but there are {} output neurons&quot;</span>,
                                   <span class='ident'>output</span>.<span class='ident'>len</span>(), <span class='ident'>num_output</span>),
            })
        }
    }

    <span class='doccomment'>/// Train with a single pair of input and output. This is always incremental training (see</span>
    <span class='doccomment'>/// `TrainAlg`), since only one pattern is presented.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>train</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>input</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>fann_type</span>], <span class='ident'>desired_output</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>fann_type</span>]) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span>()<span class='op'>&gt;</span> {
        <span class='kw'>unsafe</span> {
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='self'>self</span>.<span class='ident'>check_input_size</span>(<span class='ident'>input</span>));
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='self'>self</span>.<span class='ident'>check_output_size</span>(<span class='ident'>desired_output</span>));
            <span class='ident'>fann_sys</span>::<span class='ident'>fann_train</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>input</span>.<span class='ident'>as_ptr</span>(), <span class='ident'>desired_output</span>.<span class='ident'>as_ptr</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
        }
        <span class='prelude-val'>Ok</span>(())
    }

    <span class='doccomment'>/// Train the network on the given data set.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// # Arguments</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// * `data`                   - The training data.</span>
    <span class='doccomment'>/// * `max_epochs`             - The maximum number of training epochs.</span>
    <span class='doccomment'>/// * `epochs_between_reports` - The number of epochs between printing a status report to</span>
    <span class='doccomment'>///                              `stdout`, or `0` to print no reports.</span>
    <span class='doccomment'>/// * `desired_error`          - The desired maximum value of `get_mse` or `get_bit_fail`,</span>
    <span class='doccomment'>///                              depending on which stop function was selected.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>train_on_data</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>,
                         <span class='ident'>data</span>: <span class='kw-2'>&amp;</span><span class='ident'>TrainData</span>,
                         <span class='ident'>max_epochs</span>: <span class='ident'>c_uint</span>,
                         <span class='ident'>epochs_between_reports</span>: <span class='ident'>c_uint</span>,
                         <span class='ident'>desired_error</span>: <span class='ident'>c_float</span>) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span>()<span class='op'>&gt;</span> {
        <span class='kw'>unsafe</span> {
            <span class='ident'>fann_sys</span>::<span class='ident'>fann_train_on_data</span>(<span class='self'>self</span>.<span class='ident'>raw</span>,
                                         <span class='ident'>data</span>.<span class='ident'>get_raw</span>(),
                                         <span class='ident'>max_epochs</span>,
                                         <span class='ident'>epochs_between_reports</span>,
                                         <span class='ident'>desired_error</span>);
            <span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>)
        }
    }

    <span class='doccomment'>/// Do the same as `train_on_data` but read the training data directly from a file.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>train_on_file</span><span class='op'>&lt;</span><span class='ident'>P</span>: <span class='ident'>AsRef</span><span class='op'>&lt;</span><span class='ident'>Path</span><span class='op'>&gt;&gt;</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>,
                                         <span class='ident'>path</span>: <span class='ident'>P</span>,
                                         <span class='ident'>max_epochs</span>: <span class='ident'>c_uint</span>,
                                         <span class='ident'>epochs_between_reports</span>: <span class='ident'>c_uint</span>,
                                         <span class='ident'>desired_error</span>: <span class='ident'>c_float</span>) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span>()<span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>train</span> <span class='op'>=</span> <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>TrainData</span>::<span class='ident'>from_file</span>(<span class='ident'>path</span>));
        <span class='self'>self</span>.<span class='ident'>train_on_data</span>(<span class='kw-2'>&amp;</span><span class='ident'>train</span>, <span class='ident'>max_epochs</span>, <span class='ident'>epochs_between_reports</span>, <span class='ident'>desired_error</span>)
    }

    <span class='doccomment'>/// Train one epoch with a set of training data, i. e. each sample from the training data is</span>
    <span class='doccomment'>/// considered exactly once.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Returns the mean square error as it is calculated either before or during the actual</span>
    <span class='doccomment'>/// training. This is not the actual MSE after the training epoch, but since calculating this</span>
    <span class='doccomment'>/// will require to go through the entire training set once more, it is more than adequate to</span>
    <span class='doccomment'>/// use this value during training.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>train_epoch</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>data</span>: <span class='kw-2'>&amp;</span><span class='ident'>TrainData</span>) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>c_float</span><span class='op'>&gt;</span> {
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>mse</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_train_epoch</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>data</span>.<span class='ident'>get_raw</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='prelude-val'>Ok</span>(<span class='ident'>mse</span>)
        }
    }

    <span class='doccomment'>/// Test with a single pair of input and output. This operation updates the mean square error</span>
    <span class='doccomment'>/// but does not change the network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Returns the actual output of the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>test</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>input</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>fann_type</span>], <span class='ident'>desired_output</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>fann_type</span>])
            <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>fann_type</span><span class='op'>&gt;&gt;</span> {
        <span class='macro'>try</span><span class='macro'>!</span>(<span class='self'>self</span>.<span class='ident'>check_input_size</span>(<span class='ident'>input</span>));
        <span class='macro'>try</span><span class='macro'>!</span>(<span class='self'>self</span>.<span class='ident'>check_output_size</span>(<span class='ident'>desired_output</span>));
        <span class='kw'>let</span> <span class='ident'>num_output</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>get_num_output</span>() <span class='kw'>as</span> <span class='ident'>usize</span>;
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>result</span> <span class='op'>=</span> <span class='ident'>Vec</span>::<span class='ident'>with_capacity</span>(<span class='ident'>num_output</span>);
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>output</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_test</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>input</span>.<span class='ident'>as_ptr</span>(), <span class='ident'>desired_output</span>.<span class='ident'>as_ptr</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='ident'>copy_nonoverlapping</span>(<span class='ident'>output</span>, <span class='ident'>result</span>.<span class='ident'>as_mut_ptr</span>(), <span class='ident'>num_output</span>);
            <span class='ident'>result</span>.<span class='ident'>set_len</span>(<span class='ident'>num_output</span>);
        }
        <span class='prelude-val'>Ok</span>(<span class='ident'>result</span>)
    }

    <span class='doccomment'>/// Test with a training data set and calculate the mean square error.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>test_data</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>data</span>: <span class='kw-2'>&amp;</span><span class='ident'>TrainData</span>) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>c_float</span><span class='op'>&gt;</span> {
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>mse</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_test_data</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>data</span>.<span class='ident'>get_raw</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='prelude-val'>Ok</span>(<span class='ident'>mse</span>)
        }
    }

    <span class='doccomment'>/// Get the mean square error.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_mse</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_float</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_MSE</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Get the number of fail bits, i. e. the number of neurons which differed from the desired</span>
    <span class='doccomment'>/// output by more than the bit fail limit since the previous reset.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_bit_fail</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_uint</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_bit_fail</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Reset the mean square error and bit fail count.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>reset_mse_and_bit_fail</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_reset_MSE</span>(<span class='self'>self</span>.<span class='ident'>raw</span>); }
    }

    <span class='doccomment'>/// Run the input through the neural network and returns the output. The length of the input</span>
    <span class='doccomment'>/// must equal the number of input neurons and the length of the output will equal the number</span>
    <span class='doccomment'>/// of output neurons.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>run</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>input</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>fann_type</span>]) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>fann_type</span><span class='op'>&gt;&gt;</span> {
        <span class='macro'>try</span><span class='macro'>!</span>(<span class='self'>self</span>.<span class='ident'>check_input_size</span>(<span class='ident'>input</span>));
        <span class='kw'>let</span> <span class='ident'>num_output</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>get_num_output</span>() <span class='kw'>as</span> <span class='ident'>usize</span>;
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>result</span> <span class='op'>=</span> <span class='ident'>Vec</span>::<span class='ident'>with_capacity</span>(<span class='ident'>num_output</span>);
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>output</span> <span class='op'>=</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_run</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>input</span>.<span class='ident'>as_ptr</span>());
            <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>));
            <span class='ident'>copy_nonoverlapping</span>(<span class='ident'>output</span>, <span class='ident'>result</span>.<span class='ident'>as_mut_ptr</span>(), <span class='ident'>num_output</span>);
            <span class='ident'>result</span>.<span class='ident'>set_len</span>(<span class='ident'>num_output</span>);
        }
        <span class='prelude-val'>Ok</span>(<span class='ident'>result</span>)
    }

    <span class='doccomment'>/// Get the number of input neurons.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_num_input</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_uint</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_num_input</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Get the number of output neurons.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_num_output</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_uint</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_num_output</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Get the total number of neurons, including the bias neurons.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// E. g. a 2-4-2 network has 3 + 5 + 2 = 10 neurons (because two layers have bias neurons).</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_total_neurons</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_uint</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_total_neurons</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Get the total number of connections.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_total_connections</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_uint</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_total_connections</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Get the type of the neural network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_network_type</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>NetType</span> {
        <span class='kw'>let</span> <span class='ident'>nt_enum</span> <span class='op'>=</span> <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_network_type</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) };
        <span class='ident'>NetType</span>::<span class='ident'>from_nettype_enum</span>(<span class='ident'>nt_enum</span>)
    }

    <span class='doccomment'>/// Get the connection rate used when the network was created.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_connection_rate</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_float</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_connection_rate</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Get the number of layers in the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_num_layers</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_uint</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_num_layers</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Get the number of neurons in each layer of the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_layer_sizes</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>c_uint</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>num_layers</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>get_num_layers</span>() <span class='kw'>as</span> <span class='ident'>usize</span>;
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>result</span> <span class='op'>=</span> <span class='ident'>Vec</span>::<span class='ident'>with_capacity</span>(<span class='ident'>num_layers</span>);
        <span class='kw'>unsafe</span> {
            <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_layer_array</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>result</span>.<span class='ident'>as_mut_ptr</span>());
            <span class='ident'>result</span>.<span class='ident'>set_len</span>(<span class='ident'>num_layers</span>);
        }
        <span class='ident'>result</span>
    }

    <span class='doccomment'>/// Get the number of bias neurons in each layer of the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_bias_counts</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>c_uint</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>num_layers</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>get_num_layers</span>() <span class='kw'>as</span> <span class='ident'>usize</span>;
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>result</span> <span class='op'>=</span> <span class='ident'>Vec</span>::<span class='ident'>with_capacity</span>(<span class='ident'>num_layers</span>);
        <span class='kw'>unsafe</span> {
            <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_bias_array</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>result</span>.<span class='ident'>as_mut_ptr</span>());
            <span class='ident'>result</span>.<span class='ident'>set_len</span>(<span class='ident'>num_layers</span>);
        }
        <span class='ident'>result</span>
    }

    <span class='doccomment'>/// Get a list of all connections in the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_connections</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Connection</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>total</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>get_total_connections</span>() <span class='kw'>as</span> <span class='ident'>usize</span>;
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>result</span> <span class='op'>=</span> <span class='ident'>Vec</span>::<span class='ident'>with_capacity</span>(<span class='ident'>total</span>);
        <span class='kw'>unsafe</span> {
            <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_connection_array</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>result</span>.<span class='ident'>as_mut_ptr</span>());
            <span class='ident'>result</span>.<span class='ident'>set_len</span>(<span class='ident'>total</span>);
        }
        <span class='ident'>result</span>
    }

    <span class='doccomment'>/// Set the weights of all given connections.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Connections that don&#39;t already exist are ignored.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_connections</span><span class='op'>&lt;</span><span class='lifetime'>&#39;a</span>, <span class='ident'>I</span>: <span class='ident'>IntoIterator</span><span class='op'>&lt;</span><span class='ident'>Item</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='lifetime'>&#39;a</span> <span class='ident'>Connection</span><span class='op'>&gt;&gt;</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>connections</span>: <span class='ident'>I</span>) {
        <span class='kw'>for</span> <span class='ident'>c</span> <span class='kw'>in</span> <span class='ident'>connections</span> {
            <span class='self'>self</span>.<span class='ident'>set_weight</span>(<span class='ident'>c</span>.<span class='ident'>from_neuron</span>, <span class='ident'>c</span>.<span class='ident'>to_neuron</span>, <span class='ident'>c</span>.<span class='ident'>weight</span>);
        }
    }

    <span class='doccomment'>/// Set the weight of the given connection.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_weight</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>from_neuron</span>: <span class='ident'>c_uint</span>, <span class='ident'>to_neuron</span>: <span class='ident'>c_uint</span>, <span class='ident'>weight</span>: <span class='ident'>fann_type</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_weight</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>from_neuron</span>, <span class='ident'>to_neuron</span>, <span class='ident'>weight</span>) }
    }

    <span class='doccomment'>/// Get the activation function for neuron number `neuron` in layer number `layer`, counting</span>
    <span class='doccomment'>/// the input layer as number 0. Input layer neurons do not have an activation function, so</span>
    <span class='doccomment'>/// `layer` must be at least 1.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_activation_func</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>layer</span>: <span class='ident'>c_int</span>, <span class='ident'>neuron</span>: <span class='ident'>c_int</span>) <span class='op'>-&gt;</span> <span class='ident'>FannResult</span><span class='op'>&lt;</span><span class='ident'>ActivationFunc</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>af_enum</span> <span class='op'>=</span> <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_activation_function</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>layer</span>, <span class='ident'>neuron</span>) };
        <span class='kw'>unsafe</span> { <span class='macro'>try</span><span class='macro'>!</span>(<span class='ident'>FannError</span>::<span class='ident'>check_no_error</span>(<span class='self'>self</span>.<span class='ident'>raw</span> <span class='kw'>as</span> <span class='op'>*</span><span class='kw-2'>mut</span> <span class='ident'>fann_sys</span>::<span class='ident'>fann_error</span>)) };
        <span class='ident'>ActivationFunc</span>::<span class='ident'>from_fann_activationfunc_enum</span>(<span class='ident'>af_enum</span>)
    }

    <span class='doccomment'>/// Set the activation function for neuron number `neuron` in layer number `layer`, counting</span>
    <span class='doccomment'>/// the input layer as number 0. Input layer neurons do not have an activation function, so</span>
    <span class='doccomment'>/// `layer` must be at least 1.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_activation_func</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>af</span>: <span class='ident'>ActivationFunc</span>, <span class='ident'>layer</span>: <span class='ident'>c_int</span>, <span class='ident'>neuron</span>: <span class='ident'>c_int</span>) {
        <span class='kw'>let</span> <span class='ident'>af_enum</span> <span class='op'>=</span> <span class='ident'>af</span>.<span class='ident'>to_fann_activationfunc_enum</span>();
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_activation_function</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>af_enum</span>, <span class='ident'>layer</span>, <span class='ident'>neuron</span>) }
    }

    <span class='doccomment'>/// Set the activation function for all hidden layers.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_activation_func_hidden</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>activation_func</span>: <span class='ident'>ActivationFunc</span>) {
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>af_enum</span> <span class='op'>=</span> <span class='ident'>activation_func</span>.<span class='ident'>to_fann_activationfunc_enum</span>();
            <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_activation_function_hidden</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>af_enum</span>);
        }
    }

    <span class='doccomment'>/// Set the activation function for the output layer.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_activation_func_output</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>activation_func</span>: <span class='ident'>ActivationFunc</span>) {
        <span class='kw'>unsafe</span> {
            <span class='kw'>let</span> <span class='ident'>af_enum</span> <span class='op'>=</span> <span class='ident'>activation_func</span>.<span class='ident'>to_fann_activationfunc_enum</span>();
            <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_activation_function_output</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>af_enum</span>)
        }
    }

    <span class='doccomment'>/// Get the activation steepness for neuron number `neuron` in layer number `layer`.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_activation_steepness</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>layer</span>: <span class='ident'>c_int</span>, <span class='ident'>neuron</span>: <span class='ident'>c_int</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>fann_type</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>steepness</span> <span class='op'>=</span> <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_activation_steepness</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>layer</span>, <span class='ident'>neuron</span>) };
        <span class='kw'>match</span> <span class='ident'>steepness</span> {
            <span class='op'>-</span><span class='number'>1.0</span> <span class='op'>=&gt;</span> <span class='prelude-val'>None</span>,
            <span class='ident'>s</span>    <span class='op'>=&gt;</span> <span class='prelude-val'>Some</span>(<span class='ident'>s</span>),
        }
    }

    <span class='doccomment'>/// Set the activation steepness for neuron number `neuron` in layer number `layer`, counting</span>
    <span class='doccomment'>/// the input layer as number 0. Input layer neurons do not have an activation steepness, so</span>
    <span class='doccomment'>/// layer must be at least 1.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The steepness determines how fast the function goes from minimum to maximum. A higher value</span>
    <span class='doccomment'>/// will result in more aggressive training.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// A steep activation function is adequate if outputs are binary, e. e. they are supposed to</span>
    <span class='doccomment'>/// be either almost 0 or almost 1.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The default value is 0.5.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_activation_steepness</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>steepness</span>: <span class='ident'>fann_type</span>, <span class='ident'>layer</span>: <span class='ident'>c_int</span>, <span class='ident'>neuron</span>: <span class='ident'>c_int</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_activation_steepness</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>steepness</span>, <span class='ident'>layer</span>, <span class='ident'>neuron</span>) }
    }

    <span class='doccomment'>/// Set the activation steepness for layer number `layer`.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_activation_steepness_layer</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>steepness</span>: <span class='ident'>fann_type</span>, <span class='ident'>layer</span>: <span class='ident'>c_int</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_activation_steepness_layer</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>steepness</span>, <span class='ident'>layer</span>) }
    }

    <span class='doccomment'>/// Set the activation steepness for all hidden layers.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_activation_steepness_hidden</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>steepness</span>: <span class='ident'>fann_type</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_activation_steepness_hidden</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>steepness</span>) }
    }

    <span class='doccomment'>/// Set the activation steepness for the output layer.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_activation_steepness_output</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>steepness</span>: <span class='ident'>fann_type</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_activation_steepness_output</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>steepness</span>) }
    }

    <span class='doccomment'>/// Get the error function used during training.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_error_func</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>ErrorFunc</span> {
        <span class='kw'>let</span> <span class='ident'>ef_enum</span> <span class='op'>=</span> <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_train_error_function</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) };
        <span class='ident'>ErrorFunc</span>::<span class='ident'>from_errorfunc_enum</span>(<span class='ident'>ef_enum</span>)
    }

    <span class='doccomment'>/// Set the error function used during training.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The default is `Tanh`.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_error_func</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>ef</span>: <span class='ident'>ErrorFunc</span>) {
        <span class='kw'>let</span> <span class='ident'>ef_enum</span> <span class='op'>=</span> <span class='ident'>ef</span>.<span class='ident'>to_errorfunc_enum</span>();
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_train_error_function</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>ef_enum</span>) }
    }

    <span class='doccomment'>/// Get the stop criterion for training.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_stop_func</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>StopFunc</span> {
        <span class='kw'>let</span> <span class='ident'>sf_enum</span> <span class='op'>=</span> <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_train_stop_function</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) };
        <span class='ident'>StopFunc</span>::<span class='ident'>from_stopfunc_enum</span>(<span class='ident'>sf_enum</span>)
    }

    <span class='doccomment'>/// Set the stop criterion for training.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The default is `Mse`.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_stop_func</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>sf</span>: <span class='ident'>StopFunc</span>) {
        <span class='kw'>let</span> <span class='ident'>sf_enum</span> <span class='op'>=</span> <span class='ident'>sf</span>.<span class='ident'>to_stopfunc_enum</span>();
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_train_stop_function</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>sf_enum</span>) }
    }

    <span class='doccomment'>/// Get the bit fail limit.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_bit_fail_limit</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>fann_type</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_bit_fail_limit</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Set the bit fail limit.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Each output neuron value that differs from the desired output by more than the bit fail</span>
    <span class='doccomment'>/// limit is counted as a failed bit.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_bit_fail_limit</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>bit_fail_limit</span>: <span class='ident'>fann_type</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_bit_fail_limit</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>bit_fail_limit</span>) }
    }

    <span class='comment'>// TODO: cascadetrain methods</span>

    <span class='doccomment'>/// Get the currently configured training algorithm.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_train_algorithm</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>TrainAlgorithm</span> {
        <span class='kw'>let</span> <span class='ident'>ft_enum</span> <span class='op'>=</span> <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_training_algorithm</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) };
        <span class='kw'>match</span> <span class='ident'>ft_enum</span> {
            <span class='ident'>FANN_TRAIN_INCREMENTAL</span> <span class='op'>=&gt;</span> <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Incremental</span>,
            <span class='ident'>FANN_TRAIN_BATCH</span>       <span class='op'>=&gt;</span> <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Batch</span>,
            <span class='ident'>FANN_TRAIN_RPROP</span>       <span class='op'>=&gt;</span> <span class='kw'>unsafe</span> {
                <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Rprop</span> {
                    <span class='ident'>decrease_factor</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_rprop_decrease_factor</span>(<span class='self'>self</span>.<span class='ident'>raw</span>),
                    <span class='ident'>increase_factor</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_rprop_increase_factor</span>(<span class='self'>self</span>.<span class='ident'>raw</span>),
                    <span class='ident'>delta_min</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_rprop_delta_min</span>(<span class='self'>self</span>.<span class='ident'>raw</span>),
                    <span class='ident'>delta_max</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_rprop_delta_max</span>(<span class='self'>self</span>.<span class='ident'>raw</span>),
                    <span class='ident'>delta_zero</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_rprop_delta_zero</span>(<span class='self'>self</span>.<span class='ident'>raw</span>),
                }
            },
            <span class='ident'>FANN_TRAIN_QUICKPROP</span>   <span class='op'>=&gt;</span> <span class='kw'>unsafe</span> {
                <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Quickprop</span> {
                    <span class='ident'>decay</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_quickprop_decay</span>(<span class='self'>self</span>.<span class='ident'>raw</span>),
                    <span class='ident'>mu</span>: <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_quickprop_mu</span>(<span class='self'>self</span>.<span class='ident'>raw</span>),
                }
            },
        }
    }

    <span class='doccomment'>/// Set the algorithm to be used for training.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_train_algorithm</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>ta</span>: <span class='ident'>TrainAlgorithm</span>) {
        <span class='kw'>match</span> <span class='ident'>ta</span> {
            <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Incremental</span> <span class='op'>=&gt;</span> <span class='kw'>unsafe</span> {
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_training_algorithm</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>FANN_TRAIN_INCREMENTAL</span>);
            },
            <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Batch</span> <span class='op'>=&gt;</span> <span class='kw'>unsafe</span> {
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_training_algorithm</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>FANN_TRAIN_BATCH</span>);
            },
            <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Rprop</span> {
                <span class='ident'>decrease_factor</span>, <span class='ident'>increase_factor</span>, <span class='ident'>delta_min</span>, <span class='ident'>delta_max</span>, <span class='ident'>delta_zero</span>
            } <span class='op'>=&gt;</span> <span class='kw'>unsafe</span> {
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_training_algorithm</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>FANN_TRAIN_RPROP</span>);
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_rprop_decrease_factor</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>decrease_factor</span>);
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_rprop_increase_factor</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>increase_factor</span>);
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_rprop_delta_min</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>delta_min</span>);
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_rprop_delta_max</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>delta_max</span>);
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_rprop_delta_zero</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>delta_zero</span>);
            },
            <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Quickprop</span> { <span class='ident'>decay</span>, <span class='ident'>mu</span> } <span class='op'>=&gt;</span> <span class='kw'>unsafe</span> {
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_training_algorithm</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>FANN_TRAIN_QUICKPROP</span>);
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_quickprop_decay</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>decay</span>);
                <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_quickprop_mu</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>mu</span>);
            },
        }
    }

    <span class='doccomment'>/// Get the learning rate, which is used to determine how aggressive training should be (not</span>
    <span class='doccomment'>/// used by the RPROP algorithm). The default is 0.7.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_learning_rate</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_float</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_learning_rate</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Set the learning rate, which is used to determine how aggressive training should be (not</span>
    <span class='doccomment'>/// used by the RPROP algorithm). The default is 0.7.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_learning_rate</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>learning_rate</span>: <span class='ident'>c_float</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_learning_rate</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>learning_rate</span>) }
    }

    <span class='doccomment'>/// Get the learning momentum used in incremental training. It is recommended to use a value</span>
    <span class='doccomment'>/// between 0.0 and 1.0. The default is 1.0.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_learning_momentum</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='ident'>c_float</span> {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_get_learning_momentum</span>(<span class='self'>self</span>.<span class='ident'>raw</span>) }
    }

    <span class='doccomment'>/// Set the learning momentum used in incremental training. It is recommended to use a value</span>
    <span class='doccomment'>/// between 0.0 and 1.0. The default is 1.0.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>set_learning_momentum</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>learning_momentum</span>: <span class='ident'>c_float</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_set_learning_momentum</span>(<span class='self'>self</span>.<span class='ident'>raw</span>, <span class='ident'>learning_momentum</span>) }
    }

    <span class='comment'>// TODO: save_to_fixed?</span>
    <span class='comment'>// TODO: user_data methods?</span>
}

<span class='kw'>impl</span> <span class='ident'>Drop</span> <span class='kw'>for</span> <span class='ident'>Fann</span> {
    <span class='kw'>fn</span> <span class='ident'>drop</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>unsafe</span> { <span class='ident'>fann_sys</span>::<span class='ident'>fann_destroy</span>(<span class='self'>self</span>.<span class='ident'>raw</span>); }
    }
}

<span class='attribute'>#[<span class='ident'>cfg</span>(<span class='ident'>test</span>)]</span>
<span class='kw'>mod</span> <span class='ident'>tests</span> {
    <span class='kw'>use</span> <span class='ident'>super</span>::<span class='op'>*</span>;

    <span class='kw'>const</span> <span class='ident'>EPSILON</span>: <span class='ident'>f32</span> <span class='op'>=</span> <span class='number'>0.2</span>;

    <span class='attribute'>#[<span class='ident'>test</span>]</span>
    <span class='kw'>fn</span> <span class='ident'>test_tutorial</span>() {
        <span class='kw'>let</span> <span class='ident'>max_epochs</span> <span class='op'>=</span> <span class='number'>500000</span>;
        <span class='kw'>let</span> <span class='ident'>epochs_between_reports</span> <span class='op'>=</span> <span class='number'>1000</span>;
        <span class='kw'>let</span> <span class='ident'>desired_error</span> <span class='op'>=</span> <span class='number'>0.001</span>;
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>fann</span> <span class='op'>=</span> <span class='ident'>Fann</span>::<span class='ident'>new</span>(<span class='kw-2'>&amp;</span>[<span class='number'>2</span>, <span class='number'>3</span>, <span class='number'>1</span>]).<span class='ident'>unwrap</span>();
        <span class='ident'>fann</span>.<span class='ident'>set_activation_func_hidden</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidSymmetric</span>);
        <span class='ident'>fann</span>.<span class='ident'>set_activation_func_output</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidSymmetric</span>);
        <span class='ident'>fann</span>.<span class='ident'>train_on_file</span>(<span class='string'>&quot;test_files/xor.data&quot;</span>,
                           <span class='ident'>max_epochs</span>,
                           <span class='ident'>epochs_between_reports</span>,
                           <span class='ident'>desired_error</span>).<span class='ident'>unwrap</span>();
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>EPSILON</span> <span class='op'>&gt;</span> ( <span class='number'>1.0</span> <span class='op'>-</span> <span class='ident'>fann</span>.<span class='ident'>run</span>(<span class='kw-2'>&amp;</span>[<span class='op'>-</span><span class='number'>1.0</span>,  <span class='number'>1.0</span>]).<span class='ident'>unwrap</span>()[<span class='number'>0</span>]).<span class='ident'>abs</span>());
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>EPSILON</span> <span class='op'>&gt;</span> ( <span class='number'>1.0</span> <span class='op'>-</span> <span class='ident'>fann</span>.<span class='ident'>run</span>(<span class='kw-2'>&amp;</span>[ <span class='number'>1.0</span>, <span class='op'>-</span><span class='number'>1.0</span>]).<span class='ident'>unwrap</span>()[<span class='number'>0</span>]).<span class='ident'>abs</span>());
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>EPSILON</span> <span class='op'>&gt;</span> (<span class='op'>-</span><span class='number'>1.0</span> <span class='op'>-</span> <span class='ident'>fann</span>.<span class='ident'>run</span>(<span class='kw-2'>&amp;</span>[ <span class='number'>1.0</span>,  <span class='number'>1.0</span>]).<span class='ident'>unwrap</span>()[<span class='number'>0</span>]).<span class='ident'>abs</span>());
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>EPSILON</span> <span class='op'>&gt;</span> (<span class='op'>-</span><span class='number'>1.0</span> <span class='op'>-</span> <span class='ident'>fann</span>.<span class='ident'>run</span>(<span class='kw-2'>&amp;</span>[<span class='op'>-</span><span class='number'>1.0</span>, <span class='op'>-</span><span class='number'>1.0</span>]).<span class='ident'>unwrap</span>()[<span class='number'>0</span>]).<span class='ident'>abs</span>());
    }

    <span class='attribute'>#[<span class='ident'>test</span>]</span>
    <span class='kw'>fn</span> <span class='ident'>test_activation_func</span>() {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>fann</span> <span class='op'>=</span> <span class='ident'>Fann</span>::<span class='ident'>new</span>(<span class='kw-2'>&amp;</span>[<span class='number'>4</span>, <span class='number'>3</span>, <span class='number'>3</span>, <span class='number'>1</span>]).<span class='ident'>unwrap</span>();
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>fann</span>.<span class='ident'>get_activation_func</span>(<span class='number'>0</span>, <span class='number'>1</span>).<span class='ident'>is_err</span>());
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>fann</span>.<span class='ident'>get_activation_func</span>(<span class='number'>4</span>, <span class='number'>1</span>).<span class='ident'>is_err</span>());
        <span class='macro'>assert_eq</span><span class='macro'>!</span>(<span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>SigmoidStepwise</span>), <span class='ident'>fann</span>.<span class='ident'>get_activation_func</span>(<span class='number'>2</span>, <span class='number'>2</span>));
        <span class='ident'>fann</span>.<span class='ident'>set_activation_func</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Sin</span>, <span class='number'>2</span>, <span class='number'>2</span>);
        <span class='macro'>assert_eq</span><span class='macro'>!</span>(<span class='prelude-val'>Ok</span>(<span class='ident'>ActivationFunc</span>::<span class='ident'>Sin</span>), <span class='ident'>fann</span>.<span class='ident'>get_activation_func</span>(<span class='number'>2</span>, <span class='number'>2</span>));
    }

    <span class='attribute'>#[<span class='ident'>test</span>]</span>
    <span class='kw'>fn</span> <span class='ident'>test_train_algorithm</span>() {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>fann</span> <span class='op'>=</span> <span class='ident'>Fann</span>::<span class='ident'>new</span>(<span class='kw-2'>&amp;</span>[<span class='number'>4</span>, <span class='number'>3</span>, <span class='number'>3</span>, <span class='number'>1</span>]).<span class='ident'>unwrap</span>();
        <span class='macro'>assert_eq</span><span class='macro'>!</span>(<span class='ident'>TrainAlgorithm</span>::<span class='ident'>default</span>(), <span class='ident'>fann</span>.<span class='ident'>get_train_algorithm</span>());
        <span class='kw'>let</span> <span class='ident'>quickprop</span> <span class='op'>=</span> <span class='ident'>TrainAlgorithm</span>::<span class='ident'>Quickprop</span> {
            <span class='ident'>decay</span>: <span class='op'>-</span><span class='number'>0.0002</span>,
            <span class='ident'>mu</span>: <span class='number'>1.5</span>,
        };
        <span class='ident'>fann</span>.<span class='ident'>set_train_algorithm</span>(<span class='ident'>quickprop</span>);
        <span class='macro'>assert_eq</span><span class='macro'>!</span>(<span class='ident'>quickprop</span>, <span class='ident'>fann</span>.<span class='ident'>get_train_algorithm</span>());
    }

    <span class='attribute'>#[<span class='ident'>test</span>]</span>
    <span class='kw'>fn</span> <span class='ident'>test_layer_sizes</span>() {
        <span class='kw'>let</span> <span class='ident'>fann</span> <span class='op'>=</span> <span class='ident'>Fann</span>::<span class='ident'>new</span>(<span class='kw-2'>&amp;</span>[<span class='number'>4</span>, <span class='number'>3</span>, <span class='number'>3</span>, <span class='number'>1</span>]).<span class='ident'>unwrap</span>();
        <span class='macro'>assert_eq</span><span class='macro'>!</span>(<span class='macro'>vec</span><span class='macro'>!</span>(<span class='number'>4</span>, <span class='number'>3</span>, <span class='number'>3</span>, <span class='number'>1</span>), <span class='ident'>fann</span>.<span class='ident'>get_layer_sizes</span>());
        <span class='macro'>assert_eq</span><span class='macro'>!</span>(<span class='macro'>vec</span><span class='macro'>!</span>(<span class='number'>1</span>, <span class='number'>1</span>, <span class='number'>1</span>, <span class='number'>0</span>), <span class='ident'>fann</span>.<span class='ident'>get_bias_counts</span>());
    }

    <span class='attribute'>#[<span class='ident'>test</span>]</span>
    <span class='kw'>fn</span> <span class='ident'>test_get_set_connections</span>() {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>fann</span> <span class='op'>=</span> <span class='ident'>Fann</span>::<span class='ident'>new</span>(<span class='kw-2'>&amp;</span>[<span class='number'>1</span>, <span class='number'>1</span>]).<span class='ident'>unwrap</span>();
        <span class='kw'>let</span> <span class='ident'>connection</span> <span class='op'>=</span> <span class='ident'>Connection</span> { <span class='ident'>from_neuron</span>: <span class='number'>1</span>, <span class='ident'>to_neuron</span>: <span class='number'>2</span>, <span class='ident'>weight</span>: <span class='number'>0.123</span> };
        <span class='ident'>fann</span>.<span class='ident'>set_connections</span>(<span class='kw-2'>&amp;</span>[<span class='ident'>connection</span>]);
        <span class='macro'>assert_eq</span><span class='macro'>!</span>(<span class='number'>2</span>, <span class='ident'>fann</span>.<span class='ident'>get_total_connections</span>()); <span class='comment'>// 2 because of the bias neuron in layer 0.</span>
        <span class='comment'>// TODO (with fann-sys 0.1.2): assert_eq!(connection, fann.get_connections()[1]);</span>
    }
}
</pre>
</section>
    <section id='search' class="content hidden"></section>

    <section class="footer"></section>

    <div id="help" class="hidden">
        <div class="shortcuts">
            <h1>Keyboard shortcuts</h1>
            <dl>
                <dt>?</dt>
                <dd>Show this help dialog</dd>
                <dt>S</dt>
                <dd>Focus the search field</dd>
                <dt>&larrb;</dt>
                <dd>Move up in search results</dd>
                <dt>&rarrb;</dt>
                <dd>Move down in search results</dd>
                <dt>&#9166;</dt>
                <dd>Go to active search result</dd>
            </dl>
        </div>
        <div class="infos">
            <h1>Search tricks</h1>
            <p>
                Prefix searches with a type followed by a colon (e.g.
                <code>fn:</code>) to restrict the search to a given type.
            </p>
            <p>
                Accepted types are: <code>fn</code>, <code>mod</code>,
                <code>struct</code>, <code>enum</code>,
                <code>trait</code>, <code>typedef</code> (or
                <code>tdef</code>).
            </p>
            <p>
                Search functions by type signature (e.g.
                <code>vec -> usize</code>)
            </p>
        </div>
    </div>

    

    <script>
        window.rootPath = "../../";
        window.currentCrate = "fann";
        window.playgroundUrl = "";
    </script>
    <script src="../../jquery.js"></script>
    <script src="../../main.js"></script>
    
    <script async src="../../search-index.js"></script>
</body>
</html>